{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前言\n",
    "研一时接触过推荐系统的一些课本知识点，未有实地地构建一个推荐系统，当时需要完成一个大作业，我和几个同学选择了实现一个电影推荐系统，但是大框架的实现是在前人的肩膀上进行完善的，就是学习github上的项目。我负责的是前端的界面设计以及接口的实现，因此没有学习到推荐系统框架的构建核心，故重新学习推荐系统是如何构建的，对模型代码进行解析。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[github项目入口](https://github.com/chengstone/movie_recommender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 电影推荐系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 系统模型框架\n",
    "![](./pic1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 数据集\n",
    "下载[`数据集`](http://files.grouplens.org/datasets/movielens/ml-1m.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用户数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>OccupationID</th>\n",
       "      <th>Zip-code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID Gender  Age  OccupationID Zip-code\n",
       "0       1      F    1            10    48067\n",
       "1       2      M   56            16    70072\n",
       "2       3      M   25            15    55117\n",
       "3       4      M   45             7    02460\n",
       "4       5      M   25            20    55455"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "users_title = ['UserID', 'Gender', 'Age', 'OccupationID', 'Zip-code']\n",
    "users = pd.read_table('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')\n",
    "# head(n),默认n=5\n",
    "users.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 电影数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3883\n"
     ]
    }
   ],
   "source": [
    "movies_title = ['MovieID', 'Title', 'Genres']\n",
    "movies = pd.read_table('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
    "movies.head()\n",
    "# for val in movies['Genres'].str.split('|'):\n",
    "#     print(val)\n",
    "print(len(set(movies['Title'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 评分数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating  timestamps\n",
       "0       1     1193       5   978300760\n",
       "1       1      661       3   978302109\n",
       "2       1      914       3   978301968\n",
       "3       1     3408       4   978300275\n",
       "4       1     2355       5   978824291"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_title = ['UserID', 'MovieID', 'Rating', 'timestamps']\n",
    "ratings = pd.read_table('./ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 数据集预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- UserID、Occupation和MovieID不用变。\n",
    "- Gender字段：需要将‘F’和‘M’转换成0和1。\n",
    "- Age字段：要转成7个连续数字0~6。\n",
    "- Genres字段：是分类字段，要转成数字。首先将Genres中的类别转成字符串到数字的字典，然后再将每个电影的Genres字段转成数字列表，因为有些电影是多个Genres的组合。\n",
    "- Title字段：处理方式跟Genres字段一样，首先创建文本到数字的字典，然后将Title中的描述转成数字的列表。另外Title中的年份也需要去掉。\n",
    "- Genres和Title字段需要将长度统一，这样在神经网络中方便处理。空白部分用‘< PAD >’对应的数字填充。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 处理users数据\n",
    "   - pd.read_table()读取数据\n",
    "   - 处理Gender字段\n",
    "      - users.filter(regex='xxx')过滤未用到的zip-code\n",
    "      - users['Gender'].map(dict)将原始数据转换成数字进行处理\n",
    "   - 处理Age字段\n",
    "      - set(users['Age'])获取无序不重复的列表，得到数据集中所有不同的Age值\n",
    "      - 遍历enumerate(set())获取下标以及数据本身，并创建对应数字字典(数据:下标)。此处并非是按照年龄大小排序，只需要一个一一对应即可\n",
    "      - users['Age'].map(dict)将原始数据转换成数字进行处理\n",
    "- 处理movies数据\n",
    "   - 处理Genres字段(one-hot编码)\n",
    "      - movies['Genres'].str.split('|')将原始数据转换成列表形式\n",
    "      - set.update()将两个集合合并成一个集合，并无序不重复。此处遍历所有列表，并合并成一个不重复的集合，得到所有不同的Genres的一个集合\n",
    "      - set.add('$<PAD>$')在Genres类型集合中添加空白类型\n",
    "      - 遍历enumerate(set)，创建数字字典，获取18种电影类型以及空白类型以及对应数字的字典。\n",
    "      - 通过已求得的类型字典，创建原始Genres字段数据的列表数据对应数字列表的字典\n",
    "      - 遍历列表字典，并将不满足长度18的类型数字列表，已空白类型对应数字补全长度。此处max(genres2int.values())=18，调用.insert(loc,value)插入\n",
    "   - 处理Title字段(one-hot编码)\n",
    "      - 过滤年份，使用正则表达式re.compile()，match匹配，group提取\n",
    "      - 遍历原始数据，通过set.update()获取所有title中不同的单词，并将空白单词添加进集合中\n",
    "      - 遍历enumerate(set)，创建数字title字典\n",
    "      - 通过数字title字典，将原始title数据转换成数字列表，并补全不足长度的数字列表\n",
    "- 划分数据\n",
    "   - 合并三个表，并划分出特征表和目标表\n",
    "      - 合并表，pd.merge()\n",
    "      - 划分表，pd.DataFrame.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load Dataset from File\n",
    "    \"\"\"\n",
    "    #读取User数据\n",
    "    users_title = ['UserID', 'Gender', 'Age', 'JobID', 'Zip-code']\n",
    "    users = pd.read_table('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')\n",
    "    users = users.filter(regex='UserID|Gender|Age|JobID')\n",
    "    users_orig = users.values\n",
    "    #改变User数据中性别和年龄\n",
    "    gender_map = {'F':0, 'M':1}\n",
    "    users['Gender'] = users['Gender'].map(gender_map)\n",
    "\n",
    "    age_map = {val:ii for ii,val in enumerate(set(users['Age']))}  ## set 无序不重复列表；enumerate()返回下标和数据本身。此处不需要按年龄大小排序\n",
    "    users['Age'] = users['Age'].map(age_map)\n",
    "\n",
    "    #读取Movie数据集\n",
    "    movies_title = ['MovieID', 'Title', 'Genres']\n",
    "    movies = pd.read_table('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
    "    movies_orig = movies.values\n",
    "    #将Title中的年份去掉\n",
    "    pattern = re.compile(r'^(.*)\\((\\d+)\\)$') \n",
    "\n",
    "    title_map = {val:pattern.match(val).group(1) for ii,val in enumerate(set(movies['Title']))} # match匹配，group提取\n",
    "    movies['Title'] = movies['Title'].map(title_map)\n",
    "\n",
    "    #电影类型转数字字典\n",
    "    genres_set = set()\n",
    "    for val in movies['Genres'].str.split('|'):\n",
    "        genres_set.update(val)\n",
    "\n",
    "    genres_set.add('<PAD>') # 将空类型添加进去\n",
    "    genres2int = {val:ii for ii, val in enumerate(genres_set)}  # 每个电影类型通过enumerate转成下标对应电影类型的数字字典\n",
    "\n",
    "    #将电影类型转成等长数字列表，长度是18\n",
    "    # 每个电影对应的类型列表\n",
    "    genres_map = {val:[genres2int[row] for row in val.split('|')] for ii,val in enumerate(set(movies['Genres']))}\n",
    "\n",
    "    # 电影类型总共18种\n",
    "    for key in genres_map:\n",
    "        for cnt in range(max(genres2int.values()) - len(genres_map[key])):  # max(genres2int.values())=18\n",
    "            genres_map[key].insert(len(genres_map[key]) + cnt,genres2int['<PAD>']) # 将类型列表已空类型对应数字补全\n",
    "    \n",
    "    movies['Genres'] = movies['Genres'].map(genres_map)\n",
    "\n",
    "    #电影Title转数字字典\n",
    "    title_set = set()\n",
    "    for val in movies['Title'].str.split():\n",
    "        title_set.update(val)\n",
    "    \n",
    "    title_set.add('<PAD>')\n",
    "    title2int = {val:ii for ii, val in enumerate(title_set)}\n",
    "\n",
    "    #将电影Title转成等长数字列表，长度是15\n",
    "    title_count = 15\n",
    "    title_map = {val:[title2int[row] for row in val.split()] for ii,val in enumerate(set(movies['Title']))}\n",
    "    \n",
    "    for key in title_map:\n",
    "        for cnt in range(title_count - len(title_map[key])):\n",
    "            title_map[key].insert(len(title_map[key]) + cnt,title2int['<PAD>'])\n",
    "    \n",
    "    movies['Title'] = movies['Title'].map(title_map)\n",
    "\n",
    "    #读取评分数据集\n",
    "    ratings_title = ['UserID','MovieID', 'ratings', 'timestamps']\n",
    "    ratings = pd.read_table('./ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
    "    ratings = ratings.filter(regex='UserID|MovieID|ratings')\n",
    "\n",
    "    #合并三个表\n",
    "    data = pd.merge(pd.merge(ratings, users), movies)\n",
    "    \n",
    "    #将数据分成X和y两张表\n",
    "    target_fields = ['ratings']\n",
    "    features_pd, targets_pd = data.drop(target_fields, axis=1), data[target_fields]\n",
    "    \n",
    "    features = features_pd.values\n",
    "    targets_values = targets_pd.values\n",
    "    \n",
    "    return title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载数据并保存到本地"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- title_count：Title字段的长度（15）\n",
    "- title_set：Title文本的集合\n",
    "- genres2int：电影类型转数字的字典\n",
    "- features：是输入X\n",
    "- targets_values：是学习目标y\n",
    "- ratings：评分数据集的Pandas对象\n",
    "- users：用户数据集的Pandas对象\n",
    "- movies：电影数据的Pandas对象\n",
    "- data：三个数据集组合在一起的Pandas对象\n",
    "- movies_orig：没有做数据处理的原始电影数据\n",
    "- users_orig：没有做数据处理的原始用户数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = load_data()\n",
    "\n",
    "pickle.dump((title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 预处理后的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>JobID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  Gender  Age  JobID\n",
       "0       1       0    0     10\n",
       "1       2       1    5     16\n",
       "2       3       1    6     15\n",
       "3       4       1    2      7\n",
       "4       5       1    6     20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[4929, 2546, 523, 523, 523, 523, 523, 523, 523...</td>\n",
       "      <td>[12, 2, 4, 13, 13, 13, 13, 13, 13, 13, 13, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[3210, 523, 523, 523, 523, 523, 523, 523, 523,...</td>\n",
       "      <td>[3, 2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[961, 1707, 2645, 523, 523, 523, 523, 523, 523...</td>\n",
       "      <td>[4, 15, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[877, 4632, 335, 523, 523, 523, 523, 523, 523,...</td>\n",
       "      <td>[4, 9, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[848, 3323, 4473, 4744, 1462, 2002, 523, 523, ...</td>\n",
       "      <td>[4, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                                              Title  \\\n",
       "0        1  [4929, 2546, 523, 523, 523, 523, 523, 523, 523...   \n",
       "1        2  [3210, 523, 523, 523, 523, 523, 523, 523, 523,...   \n",
       "2        3  [961, 1707, 2645, 523, 523, 523, 523, 523, 523...   \n",
       "3        4  [877, 4632, 335, 523, 523, 523, 523, 523, 523,...   \n",
       "4        5  [848, 3323, 4473, 4744, 1462, 2002, 523, 523, ...   \n",
       "\n",
       "                                              Genres  \n",
       "0  [12, 2, 4, 13, 13, 13, 13, 13, 13, 13, 13, 13,...  \n",
       "1  [3, 2, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13,...  \n",
       "2  [4, 15, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13...  \n",
       "3  [4, 9, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,...  \n",
       "4  [4, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1,\n",
       "       [4929, 2546, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523],\n",
       "       [12, 2, 4, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 从本地读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = pickle.load(open('preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建模型前的准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_params(params):\n",
    "    \"\"\"\n",
    "    Save parameters to file\n",
    "    \"\"\"\n",
    "    pickle.dump(params, open('params.p', 'wb'))\n",
    "\n",
    "\n",
    "def load_params():\n",
    "    \"\"\"\n",
    "    Load parameters from file\n",
    "    \"\"\"\n",
    "    return pickle.load(open('params.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 编码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#嵌入矩阵的维度\n",
    "embed_dim = 32\n",
    "#用户ID个数\n",
    "uid_max = max(features.take(0,1)) + 1 # 6040\n",
    "#性别个数\n",
    "gender_max = max(features.take(2,1)) + 1 # 1 + 1 = 2\n",
    "#年龄类别个数\n",
    "age_max = max(features.take(3,1)) + 1 # 6 + 1 = 7\n",
    "#职业个数\n",
    "job_max = max(features.take(4,1)) + 1# 20 + 1 = 21\n",
    "\n",
    "#电影ID个数\n",
    "movie_id_max = max(features.take(1,1)) + 1 # 3952\n",
    "#电影类型个数\n",
    "movie_categories_max = max(genres2int.values()) + 1 # 18 + 1 = 19\n",
    "#电影名单词个数\n",
    "movie_title_max = len(title_set) # 5216\n",
    "\n",
    "#对电影类型嵌入向量做加和操作的标志，考虑过使用mean做平均，但是没实现mean\n",
    "combiner = \"sum\"\n",
    "\n",
    "#电影名长度\n",
    "sentences_size = title_count # = 15\n",
    "#文本卷积滑动窗口，分别滑动2, 3, 4, 5个单词\n",
    "window_sizes = {2, 3, 4, 5}\n",
    "#文本卷积核数量\n",
    "filter_num = 8\n",
    "\n",
    "#电影ID转下标的字典，数据集中电影ID跟下标不一致，比如第5行的数据电影ID不一定是5\n",
    "movieid2idx = {val[0]:i for i, val in enumerate(movies.values)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 超参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 5\n",
    "# Batch Size\n",
    "batch_size = 256\n",
    "\n",
    "dropout_keep = 0.5\n",
    "# Learning Rate\n",
    "learning_rate = 0.0001\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 20\n",
    "\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络模型输入的占位符：\n",
    "  - user_id: [None, 1]\n",
    "  - user_gender: [None, 1]\n",
    "  - user_age: [None, 1]\n",
    "  - user_job: [None, 1]\n",
    "  - movie_id: [None, 1]\n",
    "  - movie_categories: [None, 18]\n",
    "  - movie_title: [None, 15]\n",
    "  - target: [None, 1]\n",
    "  - learningRate\n",
    "  - dropout_keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    uid = tf.placeholder(tf.int32, [None, 1], name=\"uid\")\n",
    "    user_gender = tf.placeholder(tf.int32, [None, 1], name=\"user_gender\")\n",
    "    user_age = tf.placeholder(tf.int32, [None, 1], name=\"user_age\")\n",
    "    user_job = tf.placeholder(tf.int32, [None, 1], name=\"user_job\")\n",
    "    \n",
    "    movie_id = tf.placeholder(tf.int32, [None, 1], name=\"movie_id\")\n",
    "    movie_categories = tf.placeholder(tf.int32, [None, 18], name=\"movie_categories\")\n",
    "    movie_titles = tf.placeholder(tf.int32, [None, 15], name=\"movie_titles\")\n",
    "    targets = tf.placeholder(tf.int32, [None, 1], name=\"targets\")\n",
    "    LearningRate = tf.placeholder(tf.float32, name = \"LearningRate\")\n",
    "    dropout_keep_prob = tf.placeholder(tf.float32, name = \"dropout_keep_prob\")\n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, LearningRate, dropout_keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 嵌入层设计\n",
    "将数据转换成词向量，使得词与词产生的向量之间存在某种联系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 介绍tensorflow中的embedding_lookup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> tf.nn.embedding_lookup(params, ids, partition_strategy='mod', name=None, validate_indices=True, max_norm=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1]]\n",
      "[[0 0 0 0 1 0]\n",
      " [1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# embedding = tf.Variable(tf.random_uniform([50, 16], -1, 1))\n",
    "embedding = tf.Variable(np.identity(6, dtype=np.int32))\n",
    "input_ids = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "input_embedding = tf.nn.embedding_lookup(embedding, input_ids)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(sess.run(embedding))\n",
    "print(sess.run(input_embedding, feed_dict={input_ids: [4, 0, 2, 4, 5, 1, 3, 0]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_embedding = tf.nn.embedding_lookup(embedding, input_ids)返回的是embedding矩阵中第input_ids行的列表。  \n",
    "\n",
    "从以上简单示例可以看出，embedding将变量表现成了one-hot形式，而input_embedding = tf.nn.embedding_lookup(embedding, input_ids)就是把input_ids中给出的tensor表现成embedding中的形式。\n",
    "\n",
    "简单来说第一个矩阵是创建了一个embedding词典，第二个矩阵是通过输入的input_ids查询上部的字典得到embedding后的值。而字典是可以由用户随意创建的，上面给出的是一个one-hot字典，还可以自由创建其他字典，例如使用正态分布或均匀分布产生（0，1）的随机数创建任意维度的embedding字典\n",
    "\n",
    "也就是说 embedding_lookup是tensorflow中给出的用于以某种方式进行embedding的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义users的嵌入矩阵\n",
    "通过研究数据集中的字段类型，我们发现有一些是类别字段，通常的处理是将这些字段转成one hot编码，但是像UserID、MovieID这样的字段就会变成非常的稀疏，输入的维度急剧膨胀，这是我们不愿意见到的。  \n",
    "所以在预处理数据时将这些字段转成了数字，我们用这个数字当做嵌入矩阵的索引，在网络的第一层使用了嵌入层，维度是（N，32）和（N，16）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- uid的词向量转换：创建embedding词典[uid_max, embed_dim]=[6040, 32],通过embedding_lookup得到embedding后的嵌入矩阵[6040, 32]\n",
    "- Genres的词向量转换：...\n",
    "- age的词向量转换：...\n",
    "- job的词向量转换：..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_user_embedding(uid, user_gender, user_age, user_job):\n",
    "    with tf.name_scope(\"user_embedding\"):\n",
    "        uid_embed_matrix = tf.Variable(tf.random_uniform([uid_max, embed_dim], -1, 1), name = \"uid_embed_matrix\")\n",
    "        uid_embed_layer = tf.nn.embedding_lookup(uid_embed_matrix, uid, name = \"uid_embed_layer\")\n",
    "    \n",
    "        gender_embed_matrix = tf.Variable(tf.random_uniform([gender_max, embed_dim // 2], -1, 1), name= \"gender_embed_matrix\")\n",
    "        gender_embed_layer = tf.nn.embedding_lookup(gender_embed_matrix, user_gender, name = \"gender_embed_layer\")\n",
    "        \n",
    "        age_embed_matrix = tf.Variable(tf.random_uniform([age_max, embed_dim // 2], -1, 1), name=\"age_embed_matrix\")\n",
    "        age_embed_layer = tf.nn.embedding_lookup(age_embed_matrix, user_age, name=\"age_embed_layer\")\n",
    "        \n",
    "        job_embed_matrix = tf.Variable(tf.random_uniform([job_max, embed_dim // 2], -1, 1), name = \"job_embed_matrix\")\n",
    "        job_embed_layer = tf.nn.embedding_lookup(job_embed_matrix, user_job, name = \"job_embed_layer\")\n",
    "    return uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- return: 将uid, user_gender, user_age, user_job这些数字作为索引，在对应的embedding矩阵中检索到索引行作为词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义Movie ID的嵌入矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Movie ID的词向量转换：..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_movie_id_embed_layer(movie_id):\n",
    "    with tf.name_scope(\"movie_embedding\"):\n",
    "        movie_id_embed_matrix = tf.Variable(tf.random_uniform([movie_id_max, embed_dim], -1, 1), name = \"movie_id_embed_matrix\")\n",
    "        movie_id_embed_layer = tf.nn.embedding_lookup(movie_id_embed_matrix, movie_id, name = \"movie_id_embed_layer\")\n",
    "    return movie_id_embed_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- return: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义Movie Genres的嵌入矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "电影类型的处理要多一步，有时一个电影有多个电影类型，这样从嵌入矩阵索引出来是一个（n，32）的矩阵，因为有多个类型嘛，我们要将这个矩阵求和，变成（1，32）的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_movie_categories_layers(movie_categories):\n",
    "    with tf.name_scope(\"movie_categories_layers\"):\n",
    "        movie_categories_embed_matrix = tf.Variable(tf.random_uniform([movie_categories_max, embed_dim], -1, 1), name = \"movie_categories_embed_matrix\")\n",
    "        movie_categories_embed_layer = tf.nn.embedding_lookup(movie_categories_embed_matrix, movie_categories, name = \"movie_categories_embed_layer\")\n",
    "        if combiner == \"sum\":\n",
    "            movie_categories_embed_layer = tf.reduce_sum(movie_categories_embed_layer, axis=1, keep_dims=True)\n",
    "    #     elif combiner == \"mean\":\n",
    "\n",
    "    return movie_categories_embed_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- return: 由于movie_categories是一个数字列表，作为索引列表，从embedding矩阵中索引得到词向量矩阵，将这些词向量加和，得到一维的词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie title的文本卷积网络设计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 嵌入层\n",
    "  - movie_title_embed_matrix： 嵌入矩阵embedding[5216， 32]\n",
    "  - movie_title_embed_layer： movie_titles的词向量矩阵[15, 32]\n",
    "  - movie_title_embed_layer_expand:  增加末尾维度[16, 32]\n",
    "- 卷积网络\n",
    "  - 原始输入数据矩阵[16, 32]\n",
    "  - 卷积核权重矩阵[window_size, embed_dim, 1, filter_num]=[window_size, 32, 1, 8]\n",
    "     - [window_size, 32]权重矩阵，此处是已window_size为滑动窗口，整体往下进行卷积计算\n",
    "     - 1，表示输入的特征图只有1个\n",
    "     - 8，表示卷积核个数为8，同样输出的特征图为8个\n",
    "  - 偏置bias: tf.constant(0.1, shape=[filter_num]\n",
    "  - 卷积计算：tf.nn.conv2d(movie_title_embed_layer_expand, filter_weights, [1,1,1,1]) + bias\n",
    "  - 激活函数：tf.nn.relu()\n",
    "  - 池化层：tf.nn.max_pool(relu_layer, [1,sentences_size - window_size + 1 ,1,1], [1,1,1,1])\n",
    "     - relu_layer：经过激活函数后的矩阵值\n",
    "     - [1,sentences_size - window_size + 1 ,1,1]：以[sentences_size - window_size + 1 ,1]网格大小进行池化\n",
    "     - [1,1,1,1]：池化步长为[1, 1]\n",
    "  - dropout层\n",
    "     - dropout layer的目的是为了防止CNN 过拟合：首先，想象我们现在只训练一个特定的网络，当迭代次数增多的时候，可能出现网络对训练集拟合的很好（在训练集上loss很小），但是对验证集的拟合程度很差的情况。所以，我们有了这样的想法：可不可以让每次跌代随机的去更新网络参数（weights），引入这样的随机性就可以增加网络generalize 的能力。所以就有了dropout 。在训练的时候，我们只需要按一定的概率（retaining probability）p 来对weight layer 的参数进行随机采样，将这个子网络作为此次更新的目标网络。可以想象，如果整个网络有n个参数，那么我们可用的子网络个数为 2^n 。 并且，当n很大时，每次迭代更新 使用的子网络基本上不会重复，从而避免了某一个网络被过分的拟合到训练集上。\n",
    "     - 经过卷积池化后的矩阵shape为(8, -, -)，因此不同尺寸卷积核构造的pool_layer_lst的shape为(len(window_sizes), 8, -, -)\n",
    "     - tf.concat(pool_layer_lst, 3)表示在第4维度上连接pool_layer_lst矩阵\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_movie_cnn_layer(movie_titles):\n",
    "    #从嵌入矩阵中得到电影名对应的各个单词的嵌入向量\n",
    "    with tf.name_scope(\"movie_embedding\"):\n",
    "        movie_title_embed_matrix = tf.Variable(tf.random_uniform([movie_title_max, embed_dim], -1, 1), name = \"movie_title_embed_matrix\")\n",
    "        movie_title_embed_layer = tf.nn.embedding_lookup(movie_title_embed_matrix, movie_titles, name = \"movie_title_embed_layer\")\n",
    "        movie_title_embed_layer_expand = tf.expand_dims(movie_title_embed_layer, -1)\n",
    "    \n",
    "    #对文本嵌入层使用不同尺寸的卷积核做卷积和最大池化\n",
    "    pool_layer_lst = []\n",
    "    for window_size in window_sizes:\n",
    "        with tf.name_scope(\"movie_txt_conv_maxpool_{}\".format(window_size)):\n",
    "            filter_weights = tf.Variable(tf.truncated_normal([window_size, embed_dim, 1, filter_num],stddev=0.1),name = \"filter_weights\")\n",
    "            filter_bias = tf.Variable(tf.constant(0.1, shape=[filter_num]), name=\"filter_bias\")\n",
    "            \n",
    "            conv_layer = tf.nn.conv2d(movie_title_embed_layer_expand, filter_weights, [1,1,1,1], padding=\"VALID\", name=\"conv_layer\")\n",
    "            relu_layer = tf.nn.relu(tf.nn.bias_add(conv_layer,filter_bias), name =\"relu_layer\")\n",
    "            \n",
    "            maxpool_layer = tf.nn.max_pool(relu_layer, [1,sentences_size - window_size + 1 ,1,1], [1,1,1,1], padding=\"VALID\", name=\"maxpool_layer\")\n",
    "            pool_layer_lst.append(maxpool_layer)\n",
    "\n",
    "    #Dropout层\n",
    "    with tf.name_scope(\"pool_dropout\"):\n",
    "        pool_layer = tf.concat(pool_layer_lst, 3, name =\"pool_layer\")\n",
    "        max_num = len(window_sizes) * filter_num\n",
    "        pool_layer_flat = tf.reshape(pool_layer , [-1, 1, max_num], name = \"pool_layer_flat\")\n",
    "    \n",
    "        dropout_layer = tf.nn.dropout(pool_layer_flat, dropout_keep_prob, name = \"dropout_layer\")\n",
    "    return pool_layer_flat, dropout_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 介绍tf.concat()与tf.reshape()\n",
    "##### tf.concat([tensor1, tensor2, tensor3,...], axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat_5:0' shape=(2, 2, 4) dtype=int32>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = [[[1, 2], [2, 3]], [[4, 4], [5, 3]]]\n",
    "t2 = [[[7, 4], [8, 4]], [[2, 10], [15, 11]]]\n",
    "tf.concat([t1, t2], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于t1多维矩阵，其shape为(2, 2, 2)(判断括号的组成)。一个多维矩阵，第axis=0个维度代表最外层方括号所框下的子集，第i个维度代表内部第i个方括号所框下的子集。维度越高，括号越小。\n",
    "\n",
    "对于这种情况，我可以再解释清楚一点: \n",
    "\n",
    "对于[[ ], [ ]]和[[ ], [ ]]，低维拼接等于拿掉最外面括号，高维拼接是拿掉里面的括号(保证其他维度不变)。注意：tf.concat()拼接的张量只会改变一个维度，其他维度是保存不变的。比如两个shape为[2,3]的矩阵拼接，要么通过axis=0变成[4,3]，要么通过axis=1变成[2,6]。改变的维度索引对应axis的值。\n",
    "\n",
    "如上面例子，axis为-1，即为最内部维度，其shape变成(2, 2, 4)，将最内部的括号去掉拼接，得到[[[1, 2, 7, 4], [2, 3, 8, 4]], [[4, 4, 2, 10], [5, 3, 15, 11]]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tf.shape()\n",
    "![](./pic2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全连接层设计\n",
    "通过一个或者多个全连接层，对数据特征进行训练，最终得到用户特征和电影特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### user全连接设计\n",
    "- tf.layers.dense(inputs, units, activation)：输入数据、输出维度、激活函数--全连接层\n",
    "   - uid_embed_layer等的shape为(1, -)，全连接层输出shape为(-, 32)\n",
    "   - user_combine_layer的shape为(1, 32*4)=(1, 128)\n",
    "   - 经过第二层全连接，user_combine_layer的shape为(1，200)\n",
    "- tf.concat()：将多个词向量拼接一起\n",
    "- tf.contrib.layers.fully_connected()--全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer):\n",
    "    with tf.name_scope(\"user_fc\"):\n",
    "        #第一层全连接\n",
    "        uid_fc_layer = tf.layers.dense(uid_embed_layer, embed_dim, name = \"uid_fc_layer\", activation=tf.nn.relu)\n",
    "        gender_fc_layer = tf.layers.dense(gender_embed_layer, embed_dim, name = \"gender_fc_layer\", activation=tf.nn.relu)\n",
    "        age_fc_layer = tf.layers.dense(age_embed_layer, embed_dim, name =\"age_fc_layer\", activation=tf.nn.relu)\n",
    "        job_fc_layer = tf.layers.dense(job_embed_layer, embed_dim, name = \"job_fc_layer\", activation=tf.nn.relu)\n",
    "        \n",
    "        #第二层全连接\n",
    "        user_combine_layer = tf.concat([uid_fc_layer, gender_fc_layer, age_fc_layer, job_fc_layer], 2)  #(?, 1, 128)\n",
    "        user_combine_layer = tf.contrib.layers.fully_connected(user_combine_layer, 200, tf.tanh)  #(?, 1, 200)\n",
    "    \n",
    "        user_combine_layer_flat = tf.reshape(user_combine_layer, [-1, 200])\n",
    "    return user_combine_layer, user_combine_layer_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### movie全连接设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_movie_feature_layer(movie_id_embed_layer, movie_categories_embed_layer, dropout_layer):\n",
    "    with tf.name_scope(\"movie_fc\"):\n",
    "        #第一层全连接\n",
    "        movie_id_fc_layer = tf.layers.dense(movie_id_embed_layer, embed_dim, name = \"movie_id_fc_layer\", activation=tf.nn.relu)\n",
    "        movie_categories_fc_layer = tf.layers.dense(movie_categories_embed_layer, embed_dim, name = \"movie_categories_fc_layer\", activation=tf.nn.relu)\n",
    "    \n",
    "        #第二层全连接\n",
    "        movie_combine_layer = tf.concat([movie_id_fc_layer, movie_categories_fc_layer, dropout_layer], 2)  #(?, 1, 96)\n",
    "        movie_combine_layer = tf.contrib.layers.fully_connected(movie_combine_layer, 200, tf.tanh)  #(?, 1, 200)\n",
    "    \n",
    "        movie_combine_layer_flat = tf.reshape(movie_combine_layer, [-1, 200])\n",
    "    return movie_combine_layer, movie_combine_layer_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建计算图-拟合特征设计\n",
    "将用户特征与电影特征进行拟合评分，得到预测评分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    #获取输入占位符\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob = get_inputs()\n",
    "    #获取User的4个嵌入向量\n",
    "    uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer = get_user_embedding(uid, user_gender, user_age, user_job)\n",
    "    #得到用户特征\n",
    "    user_combine_layer, user_combine_layer_flat = get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer)\n",
    "    #获取电影ID的嵌入向量\n",
    "    movie_id_embed_layer = get_movie_id_embed_layer(movie_id)\n",
    "    #获取电影类型的嵌入向量\n",
    "    movie_categories_embed_layer = get_movie_categories_layers(movie_categories)\n",
    "    #获取电影名的特征向量\n",
    "    pool_layer_flat, dropout_layer = get_movie_cnn_layer(movie_titles)\n",
    "    #得到电影特征\n",
    "    movie_combine_layer, movie_combine_layer_flat = get_movie_feature_layer(movie_id_embed_layer, \n",
    "                                                                                movie_categories_embed_layer, \n",
    "                                                                                dropout_layer)\n",
    "    #计算出评分，要注意两个不同的方案，inference的名字（name值）是不一样的，后面做推荐时要根据name取得tensor\n",
    "    with tf.name_scope(\"inference\"):\n",
    "        #将用户特征和电影特征作为输入，经过全连接，输出一个值的方案\n",
    "#         inference_layer = tf.concat([user_combine_layer_flat, movie_combine_layer_flat], 1)  #(?, 200)\n",
    "#         inference = tf.layers.dense(inference_layer, 1,\n",
    "#                                     kernel_initializer=tf.truncated_normal_initializer(stddev=0.01), \n",
    "#                                     kernel_regularizer=tf.nn.l2_loss, name=\"inference\")\n",
    "        #简单的将用户特征和电影特征做矩阵乘法得到一个预测评分\n",
    "#        inference = tf.matmul(user_combine_layer_flat, tf.transpose(movie_combine_layer_flat))\n",
    "        inference = tf.reduce_sum(user_combine_layer_flat * movie_combine_layer_flat, axis=1)\n",
    "        inference = tf.expand_dims(inference, axis=1)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # MSE损失，将计算值回归到评分\n",
    "        cost = tf.losses.mean_squared_error(targets, inference )\n",
    "        loss = tf.reduce_mean(cost)\n",
    "    # 优化损失 \n",
    "#     train_op = tf.train.AdamOptimizer(lr).minimize(loss)  #cost\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    gradients = optimizer.compute_gradients(loss)  #cost\n",
    "    train_op = optimizer.apply_gradients(gradients, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'inference/ExpandDims:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to C:\\Users\\Tiny\\Desktop\\推荐系统\\runs\\1551329730\n",
      "\n",
      "2019-02-28T12:55:41.227300: Epoch   0 Batch    0/3125   train_loss = 17.314\n",
      "2019-02-28T12:55:42.847652: Epoch   0 Batch   20/3125   train_loss = 5.116\n",
      "2019-02-28T12:55:44.590507: Epoch   0 Batch   40/3125   train_loss = 3.131\n",
      "2019-02-28T12:55:46.243630: Epoch   0 Batch   60/3125   train_loss = 2.430\n",
      "2019-02-28T12:55:48.288286: Epoch   0 Batch   80/3125   train_loss = 2.147\n",
      "2019-02-28T12:55:49.975660: Epoch   0 Batch  100/3125   train_loss = 2.013\n",
      "2019-02-28T12:55:52.231178: Epoch   0 Batch  120/3125   train_loss = 1.807\n",
      "2019-02-28T12:55:54.042987: Epoch   0 Batch  140/3125   train_loss = 1.785\n",
      "2019-02-28T12:55:55.908550: Epoch   0 Batch  160/3125   train_loss = 1.348\n",
      "2019-02-28T12:55:57.706369: Epoch   0 Batch  180/3125   train_loss = 1.512\n",
      "2019-02-28T12:55:59.378085: Epoch   0 Batch  200/3125   train_loss = 1.567\n",
      "2019-02-28T12:56:01.074353: Epoch   0 Batch  220/3125   train_loss = 1.518\n",
      "2019-02-28T12:56:02.859181: Epoch   0 Batch  240/3125   train_loss = 1.411\n",
      "2019-02-28T12:56:04.502282: Epoch   0 Batch  260/3125   train_loss = 1.409\n",
      "2019-02-28T12:56:06.215312: Epoch   0 Batch  280/3125   train_loss = 1.517\n",
      "2019-02-28T12:56:07.796763: Epoch   0 Batch  300/3125   train_loss = 1.424\n",
      "2019-02-28T12:56:09.452588: Epoch   0 Batch  320/3125   train_loss = 1.461\n",
      "2019-02-28T12:56:11.052473: Epoch   0 Batch  340/3125   train_loss = 1.291\n",
      "2019-02-28T12:56:12.676822: Epoch   0 Batch  360/3125   train_loss = 1.363\n",
      "2019-02-28T12:56:14.348724: Epoch   0 Batch  380/3125   train_loss = 1.324\n",
      "2019-02-28T12:56:15.972924: Epoch   0 Batch  400/3125   train_loss = 1.264\n",
      "2019-02-28T12:56:17.497923: Epoch   0 Batch  420/3125   train_loss = 1.193\n",
      "2019-02-28T12:56:19.094795: Epoch   0 Batch  440/3125   train_loss = 1.376\n",
      "2019-02-28T12:56:20.760415: Epoch   0 Batch  460/3125   train_loss = 1.294\n",
      "2019-02-28T12:56:22.378651: Epoch   0 Batch  480/3125   train_loss = 1.288\n",
      "2019-02-28T12:56:23.973602: Epoch   0 Batch  500/3125   train_loss = 1.076\n",
      "2019-02-28T12:56:25.541214: Epoch   0 Batch  520/3125   train_loss = 1.260\n",
      "2019-02-28T12:56:27.090048: Epoch   0 Batch  540/3125   train_loss = 1.088\n",
      "2019-02-28T12:56:28.644140: Epoch   0 Batch  560/3125   train_loss = 1.491\n",
      "2019-02-28T12:56:30.245676: Epoch   0 Batch  580/3125   train_loss = 1.368\n",
      "2019-02-28T12:56:32.105685: Epoch   0 Batch  600/3125   train_loss = 1.430\n",
      "2019-02-28T12:56:33.709803: Epoch   0 Batch  620/3125   train_loss = 1.396\n",
      "2019-02-28T12:56:35.326157: Epoch   0 Batch  640/3125   train_loss = 1.374\n",
      "2019-02-28T12:56:36.954277: Epoch   0 Batch  660/3125   train_loss = 1.314\n",
      "2019-02-28T12:56:38.604412: Epoch   0 Batch  680/3125   train_loss = 1.203\n",
      "2019-02-28T12:56:40.154814: Epoch   0 Batch  700/3125   train_loss = 1.300\n",
      "2019-02-28T12:56:41.741771: Epoch   0 Batch  720/3125   train_loss = 1.188\n",
      "2019-02-28T12:56:43.339919: Epoch   0 Batch  740/3125   train_loss = 1.289\n",
      "2019-02-28T12:56:44.950862: Epoch   0 Batch  760/3125   train_loss = 1.374\n",
      "2019-02-28T12:56:46.502949: Epoch   0 Batch  780/3125   train_loss = 1.351\n",
      "2019-02-28T12:56:48.051089: Epoch   0 Batch  800/3125   train_loss = 1.294\n",
      "2019-02-28T12:56:49.754748: Epoch   0 Batch  820/3125   train_loss = 1.217\n",
      "2019-02-28T12:56:51.309848: Epoch   0 Batch  840/3125   train_loss = 1.200\n",
      "2019-02-28T12:56:52.895913: Epoch   0 Batch  860/3125   train_loss = 1.199\n",
      "2019-02-28T12:56:54.472057: Epoch   0 Batch  880/3125   train_loss = 1.190\n",
      "2019-02-28T12:56:56.066780: Epoch   0 Batch  900/3125   train_loss = 1.231\n",
      "2019-02-28T12:56:57.621019: Epoch   0 Batch  920/3125   train_loss = 1.338\n",
      "2019-02-28T12:56:59.210211: Epoch   0 Batch  940/3125   train_loss = 1.373\n",
      "2019-02-28T12:57:00.862757: Epoch   0 Batch  960/3125   train_loss = 1.350\n",
      "2019-02-28T12:57:02.753515: Epoch   0 Batch  980/3125   train_loss = 1.399\n",
      "2019-02-28T12:57:04.371600: Epoch   0 Batch 1000/3125   train_loss = 1.281\n",
      "2019-02-28T12:57:05.971549: Epoch   0 Batch 1020/3125   train_loss = 1.365\n",
      "2019-02-28T12:57:07.554544: Epoch   0 Batch 1040/3125   train_loss = 1.254\n",
      "2019-02-28T12:57:09.131357: Epoch   0 Batch 1060/3125   train_loss = 1.415\n",
      "2019-02-28T12:57:10.735336: Epoch   0 Batch 1080/3125   train_loss = 1.157\n",
      "2019-02-28T12:57:12.352859: Epoch   0 Batch 1100/3125   train_loss = 1.281\n",
      "2019-02-28T12:57:14.012934: Epoch   0 Batch 1120/3125   train_loss = 1.209\n",
      "2019-02-28T12:57:15.604198: Epoch   0 Batch 1140/3125   train_loss = 1.312\n",
      "2019-02-28T12:57:17.188571: Epoch   0 Batch 1160/3125   train_loss = 1.311\n",
      "2019-02-28T12:57:18.767913: Epoch   0 Batch 1180/3125   train_loss = 1.252\n",
      "2019-02-28T12:57:20.384602: Epoch   0 Batch 1200/3125   train_loss = 1.220\n",
      "2019-02-28T12:57:21.984972: Epoch   0 Batch 1220/3125   train_loss = 1.122\n",
      "2019-02-28T12:57:23.585918: Epoch   0 Batch 1240/3125   train_loss = 1.105\n",
      "2019-02-28T12:57:25.165174: Epoch   0 Batch 1260/3125   train_loss = 1.280\n",
      "2019-02-28T12:57:26.808095: Epoch   0 Batch 1280/3125   train_loss = 1.167\n",
      "2019-02-28T12:57:28.415978: Epoch   0 Batch 1300/3125   train_loss = 1.155\n",
      "2019-02-28T12:57:30.036094: Epoch   0 Batch 1320/3125   train_loss = 1.190\n",
      "2019-02-28T12:57:31.654433: Epoch   0 Batch 1340/3125   train_loss = 1.052\n",
      "2019-02-28T12:57:33.474458: Epoch   0 Batch 1360/3125   train_loss = 1.151\n",
      "2019-02-28T12:57:35.030628: Epoch   0 Batch 1380/3125   train_loss = 1.096\n",
      "2019-02-28T12:57:36.660392: Epoch   0 Batch 1400/3125   train_loss = 1.315\n",
      "2019-02-28T12:57:38.228362: Epoch   0 Batch 1420/3125   train_loss = 1.300\n",
      "2019-02-28T12:57:39.815893: Epoch   0 Batch 1440/3125   train_loss = 1.208\n",
      "2019-02-28T12:57:41.440011: Epoch   0 Batch 1460/3125   train_loss = 1.253\n",
      "2019-02-28T12:57:43.275804: Epoch   0 Batch 1480/3125   train_loss = 1.223\n",
      "2019-02-28T12:57:45.034648: Epoch   0 Batch 1500/3125   train_loss = 1.329\n",
      "2019-02-28T12:57:46.779044: Epoch   0 Batch 1520/3125   train_loss = 1.247\n",
      "2019-02-28T12:57:48.354218: Epoch   0 Batch 1540/3125   train_loss = 1.261\n",
      "2019-02-28T12:57:49.939846: Epoch   0 Batch 1560/3125   train_loss = 1.162\n",
      "2019-02-28T12:57:51.635731: Epoch   0 Batch 1580/3125   train_loss = 1.213\n",
      "2019-02-28T12:57:53.579455: Epoch   0 Batch 1600/3125   train_loss = 1.261\n",
      "2019-02-28T12:57:55.340083: Epoch   0 Batch 1620/3125   train_loss = 1.182\n",
      "2019-02-28T12:57:57.262013: Epoch   0 Batch 1640/3125   train_loss = 1.307\n",
      "2019-02-28T12:57:59.170822: Epoch   0 Batch 1660/3125   train_loss = 1.203\n",
      "2019-02-28T12:58:01.227760: Epoch   0 Batch 1680/3125   train_loss = 1.217\n",
      "2019-02-28T12:58:03.106523: Epoch   0 Batch 1700/3125   train_loss = 1.008\n",
      "2019-02-28T12:58:05.058415: Epoch   0 Batch 1720/3125   train_loss = 1.158\n",
      "2019-02-28T12:58:06.817157: Epoch   0 Batch 1740/3125   train_loss = 1.155\n",
      "2019-02-28T12:58:08.662449: Epoch   0 Batch 1760/3125   train_loss = 1.278\n",
      "2019-02-28T12:58:10.286267: Epoch   0 Batch 1780/3125   train_loss = 1.152\n",
      "2019-02-28T12:58:11.899207: Epoch   0 Batch 1800/3125   train_loss = 1.180\n",
      "2019-02-28T12:58:13.466595: Epoch   0 Batch 1820/3125   train_loss = 1.119\n",
      "2019-02-28T12:58:15.094172: Epoch   0 Batch 1840/3125   train_loss = 1.262\n",
      "2019-02-28T12:58:16.703224: Epoch   0 Batch 1860/3125   train_loss = 1.286\n",
      "2019-02-28T12:58:18.298992: Epoch   0 Batch 1880/3125   train_loss = 1.231\n",
      "2019-02-28T12:58:19.890177: Epoch   0 Batch 1900/3125   train_loss = 1.020\n",
      "2019-02-28T12:58:21.714438: Epoch   0 Batch 1920/3125   train_loss = 1.131\n",
      "2019-02-28T12:58:23.392337: Epoch   0 Batch 1940/3125   train_loss = 1.018\n",
      "2019-02-28T12:58:24.985437: Epoch   0 Batch 1960/3125   train_loss = 1.028\n",
      "2019-02-28T12:58:26.589636: Epoch   0 Batch 1980/3125   train_loss = 1.191\n",
      "2019-02-28T12:58:28.218935: Epoch   0 Batch 2000/3125   train_loss = 1.451\n",
      "2019-02-28T12:58:29.795170: Epoch   0 Batch 2020/3125   train_loss = 1.233\n",
      "2019-02-28T12:58:31.397798: Epoch   0 Batch 2040/3125   train_loss = 1.090\n",
      "2019-02-28T12:58:33.013735: Epoch   0 Batch 2060/3125   train_loss = 1.061\n",
      "2019-02-28T12:58:34.603262: Epoch   0 Batch 2080/3125   train_loss = 1.311\n",
      "2019-02-28T12:58:36.201520: Epoch   0 Batch 2100/3125   train_loss = 1.143\n",
      "2019-02-28T12:58:37.747320: Epoch   0 Batch 2120/3125   train_loss = 1.096\n",
      "2019-02-28T12:58:39.367598: Epoch   0 Batch 2140/3125   train_loss = 1.191\n",
      "2019-02-28T12:58:41.000652: Epoch   0 Batch 2160/3125   train_loss = 1.145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-28T12:58:42.589871: Epoch   0 Batch 2180/3125   train_loss = 1.153\n",
      "2019-02-28T12:58:44.188093: Epoch   0 Batch 2200/3125   train_loss = 1.074\n",
      "2019-02-28T12:58:45.731345: Epoch   0 Batch 2220/3125   train_loss = 1.091\n",
      "2019-02-28T12:58:47.384258: Epoch   0 Batch 2240/3125   train_loss = 1.015\n",
      "2019-02-28T12:58:49.065719: Epoch   0 Batch 2260/3125   train_loss = 1.187\n",
      "2019-02-28T12:58:50.757449: Epoch   0 Batch 2280/3125   train_loss = 1.147\n",
      "2019-02-28T12:58:52.336737: Epoch   0 Batch 2300/3125   train_loss = 1.167\n",
      "2019-02-28T12:58:53.938684: Epoch   0 Batch 2320/3125   train_loss = 1.277\n",
      "2019-02-28T12:58:55.575077: Epoch   0 Batch 2340/3125   train_loss = 1.153\n",
      "2019-02-28T12:58:57.132306: Epoch   0 Batch 2360/3125   train_loss = 1.173\n",
      "2019-02-28T12:58:58.771197: Epoch   0 Batch 2380/3125   train_loss = 1.101\n",
      "2019-02-28T12:59:00.370836: Epoch   0 Batch 2400/3125   train_loss = 1.277\n",
      "2019-02-28T12:59:01.913980: Epoch   0 Batch 2420/3125   train_loss = 1.127\n",
      "2019-02-28T12:59:03.591069: Epoch   0 Batch 2440/3125   train_loss = 1.294\n",
      "2019-02-28T12:59:05.355652: Epoch   0 Batch 2460/3125   train_loss = 1.089\n",
      "2019-02-28T12:59:06.932163: Epoch   0 Batch 2480/3125   train_loss = 1.187\n",
      "2019-02-28T12:59:08.503113: Epoch   0 Batch 2500/3125   train_loss = 1.252\n",
      "2019-02-28T12:59:10.088170: Epoch   0 Batch 2520/3125   train_loss = 1.103\n",
      "2019-02-28T12:59:11.691117: Epoch   0 Batch 2540/3125   train_loss = 0.990\n",
      "2019-02-28T12:59:13.299251: Epoch   0 Batch 2560/3125   train_loss = 0.950\n",
      "2019-02-28T12:59:14.902384: Epoch   0 Batch 2580/3125   train_loss = 1.157\n",
      "2019-02-28T12:59:16.616796: Epoch   0 Batch 2600/3125   train_loss = 1.159\n",
      "2019-02-28T12:59:18.193899: Epoch   0 Batch 2620/3125   train_loss = 1.026\n",
      "2019-02-28T12:59:19.788217: Epoch   0 Batch 2640/3125   train_loss = 1.095\n",
      "2019-02-28T12:59:21.409533: Epoch   0 Batch 2660/3125   train_loss = 1.225\n",
      "2019-02-28T12:59:22.990494: Epoch   0 Batch 2680/3125   train_loss = 0.995\n",
      "2019-02-28T12:59:24.593578: Epoch   0 Batch 2700/3125   train_loss = 1.157\n",
      "2019-02-28T12:59:26.132730: Epoch   0 Batch 2720/3125   train_loss = 1.101\n",
      "2019-02-28T12:59:27.837655: Epoch   0 Batch 2740/3125   train_loss = 1.227\n",
      "2019-02-28T12:59:29.469979: Epoch   0 Batch 2760/3125   train_loss = 1.152\n",
      "2019-02-28T12:59:31.042078: Epoch   0 Batch 2780/3125   train_loss = 1.064\n",
      "2019-02-28T12:59:32.946825: Epoch   0 Batch 2800/3125   train_loss = 1.327\n",
      "2019-02-28T12:59:34.552800: Epoch   0 Batch 2820/3125   train_loss = 1.312\n",
      "2019-02-28T12:59:36.110960: Epoch   0 Batch 2840/3125   train_loss = 1.164\n",
      "2019-02-28T12:59:37.860954: Epoch   0 Batch 2860/3125   train_loss = 1.137\n",
      "2019-02-28T12:59:39.469117: Epoch   0 Batch 2880/3125   train_loss = 1.148\n",
      "2019-02-28T12:59:41.055421: Epoch   0 Batch 2900/3125   train_loss = 1.113\n",
      "2019-02-28T12:59:42.679575: Epoch   0 Batch 2920/3125   train_loss = 1.183\n",
      "2019-02-28T12:59:44.233555: Epoch   0 Batch 2940/3125   train_loss = 1.138\n",
      "2019-02-28T12:59:45.843859: Epoch   0 Batch 2960/3125   train_loss = 1.213\n",
      "2019-02-28T12:59:47.431816: Epoch   0 Batch 2980/3125   train_loss = 1.134\n",
      "2019-02-28T12:59:49.017009: Epoch   0 Batch 3000/3125   train_loss = 1.156\n",
      "2019-02-28T12:59:50.649500: Epoch   0 Batch 3020/3125   train_loss = 1.215\n",
      "2019-02-28T12:59:52.201786: Epoch   0 Batch 3040/3125   train_loss = 1.110\n",
      "2019-02-28T12:59:53.782746: Epoch   0 Batch 3060/3125   train_loss = 1.142\n",
      "2019-02-28T12:59:55.410916: Epoch   0 Batch 3080/3125   train_loss = 1.225\n",
      "2019-02-28T12:59:57.015860: Epoch   0 Batch 3100/3125   train_loss = 1.220\n",
      "2019-02-28T12:59:58.644235: Epoch   0 Batch 3120/3125   train_loss = 1.002\n",
      "2019-02-28T12:59:59.021866: Epoch   0 Batch    0/781   test_loss = 1.005\n",
      "2019-02-28T12:59:59.347651: Epoch   0 Batch   20/781   test_loss = 1.099\n",
      "2019-02-28T12:59:59.622469: Epoch   0 Batch   40/781   test_loss = 1.027\n",
      "2019-02-28T12:59:59.895290: Epoch   0 Batch   60/781   test_loss = 1.275\n",
      "2019-02-28T13:00:00.176580: Epoch   0 Batch   80/781   test_loss = 1.310\n",
      "2019-02-28T13:00:00.452398: Epoch   0 Batch  100/781   test_loss = 1.292\n",
      "2019-02-28T13:00:00.722404: Epoch   0 Batch  120/781   test_loss = 1.171\n",
      "2019-02-28T13:00:00.990228: Epoch   0 Batch  140/781   test_loss = 1.129\n",
      "2019-02-28T13:00:01.257053: Epoch   0 Batch  160/781   test_loss = 1.269\n",
      "2019-02-28T13:00:01.525876: Epoch   0 Batch  180/781   test_loss = 1.228\n",
      "2019-02-28T13:00:01.790702: Epoch   0 Batch  200/781   test_loss = 1.116\n",
      "2019-02-28T13:00:02.062523: Epoch   0 Batch  220/781   test_loss = 0.941\n",
      "2019-02-28T13:00:02.340341: Epoch   0 Batch  240/781   test_loss = 1.117\n",
      "2019-02-28T13:00:02.633149: Epoch   0 Batch  260/781   test_loss = 1.137\n",
      "2019-02-28T13:00:02.956936: Epoch   0 Batch  280/781   test_loss = 1.362\n",
      "2019-02-28T13:00:03.222502: Epoch   0 Batch  300/781   test_loss = 1.093\n",
      "2019-02-28T13:00:03.492324: Epoch   0 Batch  320/781   test_loss = 1.196\n",
      "2019-02-28T13:00:03.762357: Epoch   0 Batch  340/781   test_loss = 0.835\n",
      "2019-02-28T13:00:04.037176: Epoch   0 Batch  360/781   test_loss = 1.196\n",
      "2019-02-28T13:00:04.326986: Epoch   0 Batch  380/781   test_loss = 1.095\n",
      "2019-02-28T13:00:04.601808: Epoch   0 Batch  400/781   test_loss = 1.062\n",
      "2019-02-28T13:00:04.883620: Epoch   0 Batch  420/781   test_loss = 1.006\n",
      "2019-02-28T13:00:05.154442: Epoch   0 Batch  440/781   test_loss = 1.206\n",
      "2019-02-28T13:00:05.425264: Epoch   0 Batch  460/781   test_loss = 1.113\n",
      "2019-02-28T13:00:05.700084: Epoch   0 Batch  480/781   test_loss = 1.090\n",
      "2019-02-28T13:00:05.967908: Epoch   0 Batch  500/781   test_loss = 0.961\n",
      "2019-02-28T13:00:06.238512: Epoch   0 Batch  520/781   test_loss = 1.086\n",
      "2019-02-28T13:00:06.508335: Epoch   0 Batch  540/781   test_loss = 0.986\n",
      "2019-02-28T13:00:06.784323: Epoch   0 Batch  560/781   test_loss = 1.217\n",
      "2019-02-28T13:00:07.058142: Epoch   0 Batch  580/781   test_loss = 1.145\n",
      "2019-02-28T13:00:07.322969: Epoch   0 Batch  600/781   test_loss = 1.110\n",
      "2019-02-28T13:00:07.592791: Epoch   0 Batch  620/781   test_loss = 1.122\n",
      "2019-02-28T13:00:07.860615: Epoch   0 Batch  640/781   test_loss = 1.199\n",
      "2019-02-28T13:00:08.128440: Epoch   0 Batch  660/781   test_loss = 1.054\n",
      "2019-02-28T13:00:08.402418: Epoch   0 Batch  680/781   test_loss = 1.351\n",
      "2019-02-28T13:00:08.676238: Epoch   0 Batch  700/781   test_loss = 1.035\n",
      "2019-02-28T13:00:08.945992: Epoch   0 Batch  720/781   test_loss = 1.243\n",
      "2019-02-28T13:00:09.235020: Epoch   0 Batch  740/781   test_loss = 1.087\n",
      "2019-02-28T13:00:09.509839: Epoch   0 Batch  760/781   test_loss = 1.103\n",
      "2019-02-28T13:00:09.778799: Epoch   0 Batch  780/781   test_loss = 1.044\n",
      "2019-02-28T13:00:11.660758: Epoch   1 Batch   15/3125   train_loss = 1.191\n",
      "2019-02-28T13:00:13.226665: Epoch   1 Batch   35/3125   train_loss = 1.143\n",
      "2019-02-28T13:00:14.782643: Epoch   1 Batch   55/3125   train_loss = 1.145\n",
      "2019-02-28T13:00:16.346788: Epoch   1 Batch   75/3125   train_loss = 1.112\n",
      "2019-02-28T13:00:17.913758: Epoch   1 Batch   95/3125   train_loss = 1.003\n",
      "2019-02-28T13:00:19.470629: Epoch   1 Batch  115/3125   train_loss = 1.146\n",
      "2019-02-28T13:00:21.026346: Epoch   1 Batch  135/3125   train_loss = 1.017\n",
      "2019-02-28T13:00:22.615626: Epoch   1 Batch  155/3125   train_loss = 1.075\n",
      "2019-02-28T13:00:24.204751: Epoch   1 Batch  175/3125   train_loss = 1.074\n",
      "2019-02-28T13:00:25.822179: Epoch   1 Batch  195/3125   train_loss = 1.183\n",
      "2019-02-28T13:00:27.438386: Epoch   1 Batch  215/3125   train_loss = 1.097\n",
      "2019-02-28T13:00:28.981056: Epoch   1 Batch  235/3125   train_loss = 1.006\n",
      "2019-02-28T13:00:30.594428: Epoch   1 Batch  255/3125   train_loss = 1.176\n",
      "2019-02-28T13:00:32.340771: Epoch   1 Batch  275/3125   train_loss = 0.968\n",
      "2019-02-28T13:00:33.988129: Epoch   1 Batch  295/3125   train_loss = 0.952\n",
      "2019-02-28T13:00:35.542611: Epoch   1 Batch  315/3125   train_loss = 1.045\n",
      "2019-02-28T13:00:37.125899: Epoch   1 Batch  335/3125   train_loss = 0.960\n",
      "2019-02-28T13:00:38.668999: Epoch   1 Batch  355/3125   train_loss = 1.073\n",
      "2019-02-28T13:00:40.248826: Epoch   1 Batch  375/3125   train_loss = 1.184\n",
      "2019-02-28T13:00:41.809802: Epoch   1 Batch  395/3125   train_loss = 1.010\n",
      "2019-02-28T13:00:43.365939: Epoch   1 Batch  415/3125   train_loss = 1.267\n",
      "2019-02-28T13:00:44.923915: Epoch   1 Batch  435/3125   train_loss = 1.084\n",
      "2019-02-28T13:00:46.496172: Epoch   1 Batch  455/3125   train_loss = 1.090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-28T13:00:48.085694: Epoch   1 Batch  475/3125   train_loss = 1.083\n",
      "2019-02-28T13:00:49.775584: Epoch   1 Batch  495/3125   train_loss = 1.093\n",
      "2019-02-28T13:00:51.395245: Epoch   1 Batch  515/3125   train_loss = 1.142\n",
      "2019-02-28T13:00:52.981370: Epoch   1 Batch  535/3125   train_loss = 1.099\n",
      "2019-02-28T13:00:54.538541: Epoch   1 Batch  555/3125   train_loss = 1.167\n",
      "2019-02-28T13:00:56.111981: Epoch   1 Batch  575/3125   train_loss = 1.045\n",
      "2019-02-28T13:00:57.721354: Epoch   1 Batch  595/3125   train_loss = 1.261\n",
      "2019-02-28T13:00:59.344444: Epoch   1 Batch  615/3125   train_loss = 1.045\n",
      "2019-02-28T13:01:00.952329: Epoch   1 Batch  635/3125   train_loss = 1.106\n",
      "2019-02-28T13:01:02.679194: Epoch   1 Batch  655/3125   train_loss = 0.939\n",
      "2019-02-28T13:01:04.320648: Epoch   1 Batch  675/3125   train_loss = 0.905\n",
      "2019-02-28T13:01:05.877625: Epoch   1 Batch  695/3125   train_loss = 1.056\n",
      "2019-02-28T13:01:07.473927: Epoch   1 Batch  715/3125   train_loss = 1.109\n",
      "2019-02-28T13:01:09.062768: Epoch   1 Batch  735/3125   train_loss = 0.953\n",
      "2019-02-28T13:01:10.706297: Epoch   1 Batch  755/3125   train_loss = 1.173\n",
      "2019-02-28T13:01:12.290151: Epoch   1 Batch  775/3125   train_loss = 0.983\n",
      "2019-02-28T13:01:13.810208: Epoch   1 Batch  795/3125   train_loss = 1.190\n",
      "2019-02-28T13:01:15.414507: Epoch   1 Batch  815/3125   train_loss = 1.062\n",
      "2019-02-28T13:01:17.020543: Epoch   1 Batch  835/3125   train_loss = 1.003\n",
      "2019-02-28T13:01:18.646390: Epoch   1 Batch  855/3125   train_loss = 1.304\n",
      "2019-02-28T13:01:20.241788: Epoch   1 Batch  875/3125   train_loss = 1.071\n",
      "2019-02-28T13:01:21.833368: Epoch   1 Batch  895/3125   train_loss = 1.023\n",
      "2019-02-28T13:01:23.413330: Epoch   1 Batch  915/3125   train_loss = 1.048\n",
      "2019-02-28T13:01:24.988365: Epoch   1 Batch  935/3125   train_loss = 1.199\n",
      "2019-02-28T13:01:26.546978: Epoch   1 Batch  955/3125   train_loss = 1.181\n",
      "2019-02-28T13:01:28.140327: Epoch   1 Batch  975/3125   train_loss = 1.141\n",
      "2019-02-28T13:01:29.737217: Epoch   1 Batch  995/3125   train_loss = 0.896\n",
      "2019-02-28T13:01:31.343556: Epoch   1 Batch 1015/3125   train_loss = 1.076\n",
      "2019-02-28T13:01:33.031569: Epoch   1 Batch 1035/3125   train_loss = 1.099\n",
      "2019-02-28T13:01:34.734641: Epoch   1 Batch 1055/3125   train_loss = 1.039\n",
      "2019-02-28T13:01:36.351869: Epoch   1 Batch 1075/3125   train_loss = 1.089\n",
      "2019-02-28T13:01:37.976802: Epoch   1 Batch 1095/3125   train_loss = 0.916\n",
      "2019-02-28T13:01:39.593124: Epoch   1 Batch 1115/3125   train_loss = 1.152\n",
      "2019-02-28T13:01:41.177302: Epoch   1 Batch 1135/3125   train_loss = 0.988\n",
      "2019-02-28T13:01:42.743900: Epoch   1 Batch 1155/3125   train_loss = 1.217\n",
      "2019-02-28T13:01:44.315868: Epoch   1 Batch 1175/3125   train_loss = 1.105\n",
      "2019-02-28T13:01:45.865143: Epoch   1 Batch 1195/3125   train_loss = 1.231\n",
      "2019-02-28T13:01:47.632983: Epoch   1 Batch 1215/3125   train_loss = 0.929\n",
      "2019-02-28T13:01:49.224852: Epoch   1 Batch 1235/3125   train_loss = 1.085\n",
      "2019-02-28T13:01:50.813950: Epoch   1 Batch 1255/3125   train_loss = 0.952\n",
      "2019-02-28T13:01:52.413057: Epoch   1 Batch 1275/3125   train_loss = 1.011\n",
      "2019-02-28T13:01:54.010009: Epoch   1 Batch 1295/3125   train_loss = 0.989\n",
      "2019-02-28T13:01:55.670656: Epoch   1 Batch 1315/3125   train_loss = 1.161\n",
      "2019-02-28T13:01:57.531433: Epoch   1 Batch 1335/3125   train_loss = 1.067\n",
      "2019-02-28T13:01:59.151069: Epoch   1 Batch 1355/3125   train_loss = 1.090\n",
      "2019-02-28T13:02:00.713093: Epoch   1 Batch 1375/3125   train_loss = 1.157\n",
      "2019-02-28T13:02:02.336027: Epoch   1 Batch 1395/3125   train_loss = 1.055\n",
      "2019-02-28T13:02:03.950272: Epoch   1 Batch 1415/3125   train_loss = 1.042\n",
      "2019-02-28T13:02:05.515242: Epoch   1 Batch 1435/3125   train_loss = 1.139\n",
      "2019-02-28T13:02:07.065336: Epoch   1 Batch 1455/3125   train_loss = 1.237\n",
      "2019-02-28T13:02:08.649495: Epoch   1 Batch 1475/3125   train_loss = 1.133\n",
      "2019-02-28T13:02:10.255491: Epoch   1 Batch 1495/3125   train_loss = 1.030\n",
      "2019-02-28T13:02:11.868432: Epoch   1 Batch 1515/3125   train_loss = 0.951\n",
      "2019-02-28T13:02:13.469194: Epoch   1 Batch 1535/3125   train_loss = 0.906\n",
      "2019-02-28T13:02:15.034125: Epoch   1 Batch 1555/3125   train_loss = 1.092\n",
      "2019-02-28T13:02:16.646226: Epoch   1 Batch 1575/3125   train_loss = 1.063\n",
      "2019-02-28T13:02:18.401285: Epoch   1 Batch 1595/3125   train_loss = 1.097\n",
      "2019-02-28T13:02:20.064294: Epoch   1 Batch 1615/3125   train_loss = 1.081\n",
      "2019-02-28T13:02:21.608606: Epoch   1 Batch 1635/3125   train_loss = 1.172\n",
      "2019-02-28T13:02:23.174704: Epoch   1 Batch 1655/3125   train_loss = 1.102\n",
      "2019-02-28T13:02:24.762998: Epoch   1 Batch 1675/3125   train_loss = 0.973\n",
      "2019-02-28T13:02:26.358174: Epoch   1 Batch 1695/3125   train_loss = 1.039\n",
      "2019-02-28T13:02:27.961741: Epoch   1 Batch 1715/3125   train_loss = 1.053\n",
      "2019-02-28T13:02:29.530647: Epoch   1 Batch 1735/3125   train_loss = 1.185\n",
      "2019-02-28T13:02:31.154449: Epoch   1 Batch 1755/3125   train_loss = 1.047\n",
      "2019-02-28T13:02:32.756396: Epoch   1 Batch 1775/3125   train_loss = 1.057\n",
      "2019-02-28T13:02:34.428440: Epoch   1 Batch 1795/3125   train_loss = 1.014\n",
      "2019-02-28T13:02:36.060911: Epoch   1 Batch 1815/3125   train_loss = 1.059\n",
      "2019-02-28T13:02:37.586762: Epoch   1 Batch 1835/3125   train_loss = 1.119\n",
      "2019-02-28T13:02:39.158865: Epoch   1 Batch 1855/3125   train_loss = 0.971\n",
      "2019-02-28T13:02:40.733814: Epoch   1 Batch 1875/3125   train_loss = 1.077\n",
      "2019-02-28T13:02:42.334038: Epoch   1 Batch 1895/3125   train_loss = 0.976\n",
      "2019-02-28T13:02:43.961968: Epoch   1 Batch 1915/3125   train_loss = 0.874\n",
      "2019-02-28T13:02:45.542695: Epoch   1 Batch 1935/3125   train_loss = 1.001\n",
      "2019-02-28T13:02:47.140784: Epoch   1 Batch 1955/3125   train_loss = 1.028\n",
      "2019-02-28T13:02:48.809498: Epoch   1 Batch 1975/3125   train_loss = 1.030\n",
      "2019-02-28T13:02:50.504719: Epoch   1 Batch 1995/3125   train_loss = 1.165\n",
      "2019-02-28T13:02:52.102132: Epoch   1 Batch 2015/3125   train_loss = 1.157\n",
      "2019-02-28T13:02:53.674099: Epoch   1 Batch 2035/3125   train_loss = 1.121\n",
      "2019-02-28T13:02:55.323224: Epoch   1 Batch 2055/3125   train_loss = 0.986\n",
      "2019-02-28T13:02:56.900383: Epoch   1 Batch 2075/3125   train_loss = 1.154\n",
      "2019-02-28T13:02:58.471817: Epoch   1 Batch 2095/3125   train_loss = 0.904\n",
      "2019-02-28T13:03:00.110885: Epoch   1 Batch 2115/3125   train_loss = 1.121\n",
      "2019-02-28T13:03:01.773894: Epoch   1 Batch 2135/3125   train_loss = 1.015\n",
      "2019-02-28T13:03:03.537828: Epoch   1 Batch 2155/3125   train_loss = 1.041\n",
      "2019-02-28T13:03:05.226861: Epoch   1 Batch 2175/3125   train_loss = 1.073\n",
      "2019-02-28T13:03:06.875621: Epoch   1 Batch 2195/3125   train_loss = 1.022\n",
      "2019-02-28T13:03:08.433065: Epoch   1 Batch 2215/3125   train_loss = 1.043\n",
      "2019-02-28T13:03:10.056491: Epoch   1 Batch 2235/3125   train_loss = 1.095\n",
      "2019-02-28T13:03:11.636453: Epoch   1 Batch 2255/3125   train_loss = 1.123\n",
      "2019-02-28T13:03:13.238784: Epoch   1 Batch 2275/3125   train_loss = 0.878\n",
      "2019-02-28T13:03:14.826914: Epoch   1 Batch 2295/3125   train_loss = 1.224\n",
      "2019-02-28T13:03:16.433196: Epoch   1 Batch 2315/3125   train_loss = 1.163\n",
      "2019-02-28T13:03:18.070763: Epoch   1 Batch 2335/3125   train_loss = 1.087\n",
      "2019-02-28T13:03:19.678491: Epoch   1 Batch 2355/3125   train_loss = 1.092\n",
      "2019-02-28T13:03:21.273842: Epoch   1 Batch 2375/3125   train_loss = 1.196\n",
      "2019-02-28T13:03:22.882927: Epoch   1 Batch 2395/3125   train_loss = 1.032\n",
      "2019-02-28T13:03:24.451165: Epoch   1 Batch 2415/3125   train_loss = 1.119\n",
      "2019-02-28T13:03:26.015472: Epoch   1 Batch 2435/3125   train_loss = 0.935\n",
      "2019-02-28T13:03:27.587677: Epoch   1 Batch 2455/3125   train_loss = 1.089\n",
      "2019-02-28T13:03:29.233427: Epoch   1 Batch 2475/3125   train_loss = 1.025\n",
      "2019-02-28T13:03:30.843550: Epoch   1 Batch 2495/3125   train_loss = 0.998\n",
      "2019-02-28T13:03:32.420514: Epoch   1 Batch 2515/3125   train_loss = 1.063\n",
      "2019-02-28T13:03:34.340808: Epoch   1 Batch 2535/3125   train_loss = 1.036\n",
      "2019-02-28T13:03:36.007193: Epoch   1 Batch 2555/3125   train_loss = 0.954\n",
      "2019-02-28T13:03:37.628796: Epoch   1 Batch 2575/3125   train_loss = 0.931\n",
      "2019-02-28T13:03:39.259226: Epoch   1 Batch 2595/3125   train_loss = 0.918\n",
      "2019-02-28T13:03:40.872905: Epoch   1 Batch 2615/3125   train_loss = 1.179\n",
      "2019-02-28T13:03:42.486087: Epoch   1 Batch 2635/3125   train_loss = 1.011\n",
      "2019-02-28T13:03:44.105221: Epoch   1 Batch 2655/3125   train_loss = 1.021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-28T13:03:45.709444: Epoch   1 Batch 2675/3125   train_loss = 1.029\n",
      "2019-02-28T13:03:47.281411: Epoch   1 Batch 2695/3125   train_loss = 0.992\n",
      "2019-02-28T13:03:48.855379: Epoch   1 Batch 2715/3125   train_loss = 0.991\n",
      "2019-02-28T13:03:50.461486: Epoch   1 Batch 2735/3125   train_loss = 0.829\n",
      "2019-02-28T13:03:52.034174: Epoch   1 Batch 2755/3125   train_loss = 1.038\n",
      "2019-02-28T13:03:53.651112: Epoch   1 Batch 2775/3125   train_loss = 1.134\n",
      "2019-02-28T13:03:55.249497: Epoch   1 Batch 2795/3125   train_loss = 0.996\n",
      "2019-02-28T13:03:56.826680: Epoch   1 Batch 2815/3125   train_loss = 0.945\n",
      "2019-02-28T13:03:58.450263: Epoch   1 Batch 2835/3125   train_loss = 1.027\n",
      "2019-02-28T13:04:00.068001: Epoch   1 Batch 2855/3125   train_loss = 1.098\n",
      "2019-02-28T13:04:01.657402: Epoch   1 Batch 2875/3125   train_loss = 1.043\n",
      "2019-02-28T13:04:03.235896: Epoch   1 Batch 2895/3125   train_loss = 1.075\n",
      "2019-02-28T13:04:04.899257: Epoch   1 Batch 2915/3125   train_loss = 0.953\n",
      "2019-02-28T13:04:06.494367: Epoch   1 Batch 2935/3125   train_loss = 1.111\n",
      "2019-02-28T13:04:08.125524: Epoch   1 Batch 2955/3125   train_loss = 1.060\n",
      "2019-02-28T13:04:09.714669: Epoch   1 Batch 2975/3125   train_loss = 1.008\n",
      "2019-02-28T13:04:11.319408: Epoch   1 Batch 2995/3125   train_loss = 0.968\n",
      "2019-02-28T13:04:12.936751: Epoch   1 Batch 3015/3125   train_loss = 0.978\n",
      "2019-02-28T13:04:14.545694: Epoch   1 Batch 3035/3125   train_loss = 1.026\n",
      "2019-02-28T13:04:16.106249: Epoch   1 Batch 3055/3125   train_loss = 1.170\n",
      "2019-02-28T13:04:17.760161: Epoch   1 Batch 3075/3125   train_loss = 0.981\n",
      "2019-02-28T13:04:19.509000: Epoch   1 Batch 3095/3125   train_loss = 1.003\n",
      "2019-02-28T13:04:21.147364: Epoch   1 Batch 3115/3125   train_loss = 0.912\n",
      "2019-02-28T13:04:22.189496: Epoch   1 Batch   19/781   test_loss = 1.033\n",
      "2019-02-28T13:04:22.468312: Epoch   1 Batch   39/781   test_loss = 0.867\n",
      "2019-02-28T13:04:22.737137: Epoch   1 Batch   59/781   test_loss = 0.931\n",
      "2019-02-28T13:04:23.012955: Epoch   1 Batch   79/781   test_loss = 1.055\n",
      "2019-02-28T13:04:23.283776: Epoch   1 Batch   99/781   test_loss = 0.947\n",
      "2019-02-28T13:04:23.555599: Epoch   1 Batch  119/781   test_loss = 0.918\n",
      "2019-02-28T13:04:23.822423: Epoch   1 Batch  139/781   test_loss = 0.994\n",
      "2019-02-28T13:04:24.097405: Epoch   1 Batch  159/781   test_loss = 1.038\n",
      "2019-02-28T13:04:24.379219: Epoch   1 Batch  179/781   test_loss = 0.922\n",
      "2019-02-28T13:04:24.652041: Epoch   1 Batch  199/781   test_loss = 0.954\n",
      "2019-02-28T13:04:24.926859: Epoch   1 Batch  219/781   test_loss = 1.015\n",
      "2019-02-28T13:04:25.196682: Epoch   1 Batch  239/781   test_loss = 1.195\n",
      "2019-02-28T13:04:25.471773: Epoch   1 Batch  259/781   test_loss = 0.975\n",
      "2019-02-28T13:04:25.747591: Epoch   1 Batch  279/781   test_loss = 1.143\n",
      "2019-02-28T13:04:26.020412: Epoch   1 Batch  299/781   test_loss = 1.181\n",
      "2019-02-28T13:04:26.287236: Epoch   1 Batch  319/781   test_loss = 0.994\n",
      "2019-02-28T13:04:26.557059: Epoch   1 Batch  339/781   test_loss = 0.899\n",
      "2019-02-28T13:04:26.858861: Epoch   1 Batch  359/781   test_loss = 0.940\n",
      "2019-02-28T13:04:27.142932: Epoch   1 Batch  379/781   test_loss = 1.047\n",
      "2019-02-28T13:04:27.437739: Epoch   1 Batch  399/781   test_loss = 0.837\n",
      "2019-02-28T13:04:27.747666: Epoch   1 Batch  419/781   test_loss = 0.988\n",
      "2019-02-28T13:04:28.019487: Epoch   1 Batch  439/781   test_loss = 0.996\n",
      "2019-02-28T13:04:28.295307: Epoch   1 Batch  459/781   test_loss = 1.098\n",
      "2019-02-28T13:04:28.573123: Epoch   1 Batch  479/781   test_loss = 1.073\n",
      "2019-02-28T13:04:28.850845: Epoch   1 Batch  499/781   test_loss = 0.953\n",
      "2019-02-28T13:04:29.132661: Epoch   1 Batch  519/781   test_loss = 1.042\n",
      "2019-02-28T13:04:29.397486: Epoch   1 Batch  539/781   test_loss = 0.882\n",
      "2019-02-28T13:04:29.673305: Epoch   1 Batch  559/781   test_loss = 1.189\n",
      "2019-02-28T13:04:29.945127: Epoch   1 Batch  579/781   test_loss = 1.045\n",
      "2019-02-28T13:04:30.214236: Epoch   1 Batch  599/781   test_loss = 0.969\n",
      "2019-02-28T13:04:30.498049: Epoch   1 Batch  619/781   test_loss = 1.101\n",
      "2019-02-28T13:04:30.774007: Epoch   1 Batch  639/781   test_loss = 0.862\n",
      "2019-02-28T13:04:31.044828: Epoch   1 Batch  659/781   test_loss = 1.125\n",
      "2019-02-28T13:04:31.314651: Epoch   1 Batch  679/781   test_loss = 1.155\n",
      "2019-02-28T13:04:31.592468: Epoch   1 Batch  699/781   test_loss = 0.879\n",
      "2019-02-28T13:04:31.862291: Epoch   1 Batch  719/781   test_loss = 1.002\n",
      "2019-02-28T13:04:32.143106: Epoch   1 Batch  739/781   test_loss = 1.030\n",
      "2019-02-28T13:04:32.419924: Epoch   1 Batch  759/781   test_loss = 0.888\n",
      "2019-02-28T13:04:32.691746: Epoch   1 Batch  779/781   test_loss = 0.786\n",
      "2019-02-28T13:04:34.199212: Epoch   2 Batch   10/3125   train_loss = 0.952\n",
      "2019-02-28T13:04:35.874260: Epoch   2 Batch   30/3125   train_loss = 1.039\n",
      "2019-02-28T13:04:37.421608: Epoch   2 Batch   50/3125   train_loss = 1.109\n",
      "2019-02-28T13:04:38.968252: Epoch   2 Batch   70/3125   train_loss = 1.079\n",
      "2019-02-28T13:04:40.581807: Epoch   2 Batch   90/3125   train_loss = 1.007\n",
      "2019-02-28T13:04:42.205004: Epoch   2 Batch  110/3125   train_loss = 0.909\n",
      "2019-02-28T13:04:43.810093: Epoch   2 Batch  130/3125   train_loss = 0.978\n",
      "2019-02-28T13:04:45.426201: Epoch   2 Batch  150/3125   train_loss = 1.100\n",
      "2019-02-28T13:04:47.030018: Epoch   2 Batch  170/3125   train_loss = 0.978\n",
      "2019-02-28T13:04:48.697222: Epoch   2 Batch  190/3125   train_loss = 1.032\n",
      "2019-02-28T13:04:50.428925: Epoch   2 Batch  210/3125   train_loss = 0.913\n",
      "2019-02-28T13:04:52.029322: Epoch   2 Batch  230/3125   train_loss = 0.999\n",
      "2019-02-28T13:04:53.573304: Epoch   2 Batch  250/3125   train_loss = 0.989\n",
      "2019-02-28T13:04:55.322281: Epoch   2 Batch  270/3125   train_loss = 0.818\n",
      "2019-02-28T13:04:56.943384: Epoch   2 Batch  290/3125   train_loss = 1.042\n",
      "2019-02-28T13:04:58.534591: Epoch   2 Batch  310/3125   train_loss = 0.978\n",
      "2019-02-28T13:05:00.137147: Epoch   2 Batch  330/3125   train_loss = 1.053\n",
      "2019-02-28T13:05:01.899131: Epoch   2 Batch  350/3125   train_loss = 0.929\n",
      "2019-02-28T13:05:03.533221: Epoch   2 Batch  370/3125   train_loss = 1.132\n",
      "2019-02-28T13:05:05.081368: Epoch   2 Batch  390/3125   train_loss = 1.146\n",
      "2019-02-28T13:05:06.722585: Epoch   2 Batch  410/3125   train_loss = 0.912\n",
      "2019-02-28T13:05:08.315347: Epoch   2 Batch  430/3125   train_loss = 1.200\n",
      "2019-02-28T13:05:09.921234: Epoch   2 Batch  450/3125   train_loss = 0.976\n",
      "2019-02-28T13:05:11.497198: Epoch   2 Batch  470/3125   train_loss = 0.959\n",
      "2019-02-28T13:05:13.095370: Epoch   2 Batch  490/3125   train_loss = 1.071\n",
      "2019-02-28T13:05:14.666336: Epoch   2 Batch  510/3125   train_loss = 1.100\n",
      "2019-02-28T13:05:16.266857: Epoch   2 Batch  530/3125   train_loss = 0.948\n",
      "2019-02-28T13:05:17.896785: Epoch   2 Batch  550/3125   train_loss = 0.999\n",
      "2019-02-28T13:05:19.470129: Epoch   2 Batch  570/3125   train_loss = 1.095\n",
      "2019-02-28T13:05:21.105288: Epoch   2 Batch  590/3125   train_loss = 1.036\n",
      "2019-02-28T13:05:22.735224: Epoch   2 Batch  610/3125   train_loss = 1.004\n",
      "2019-02-28T13:05:24.302349: Epoch   2 Batch  630/3125   train_loss = 1.025\n",
      "2019-02-28T13:05:25.872487: Epoch   2 Batch  650/3125   train_loss = 1.094\n",
      "2019-02-28T13:05:27.480678: Epoch   2 Batch  670/3125   train_loss = 0.930\n",
      "2019-02-28T13:05:29.048143: Epoch   2 Batch  690/3125   train_loss = 0.935\n",
      "2019-02-28T13:05:30.652874: Epoch   2 Batch  710/3125   train_loss = 0.955\n",
      "2019-02-28T13:05:32.273809: Epoch   2 Batch  730/3125   train_loss = 0.800\n",
      "2019-02-28T13:05:34.020937: Epoch   2 Batch  750/3125   train_loss = 1.008\n",
      "2019-02-28T13:05:35.707975: Epoch   2 Batch  770/3125   train_loss = 0.910\n",
      "2019-02-28T13:05:37.287610: Epoch   2 Batch  790/3125   train_loss = 0.840\n",
      "2019-02-28T13:05:38.897622: Epoch   2 Batch  810/3125   train_loss = 0.786\n",
      "2019-02-28T13:05:40.538086: Epoch   2 Batch  830/3125   train_loss = 0.870\n",
      "2019-02-28T13:05:42.154258: Epoch   2 Batch  850/3125   train_loss = 1.111\n",
      "2019-02-28T13:05:43.739924: Epoch   2 Batch  870/3125   train_loss = 0.952\n",
      "2019-02-28T13:05:45.296061: Epoch   2 Batch  890/3125   train_loss = 0.946\n",
      "2019-02-28T13:05:47.034058: Epoch   2 Batch  910/3125   train_loss = 1.037\n",
      "2019-02-28T13:05:48.609945: Epoch   2 Batch  930/3125   train_loss = 1.041\n",
      "2019-02-28T13:05:50.221223: Epoch   2 Batch  950/3125   train_loss = 0.916\n",
      "2019-02-28T13:05:51.861314: Epoch   2 Batch  970/3125   train_loss = 1.066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-28T13:05:53.496855: Epoch   2 Batch  990/3125   train_loss = 0.854\n",
      "2019-02-28T13:05:55.083469: Epoch   2 Batch 1010/3125   train_loss = 1.174\n",
      "2019-02-28T13:05:56.679455: Epoch   2 Batch 1030/3125   train_loss = 0.932\n",
      "2019-02-28T13:05:58.323895: Epoch   2 Batch 1050/3125   train_loss = 0.900\n",
      "2019-02-28T13:05:59.973060: Epoch   2 Batch 1070/3125   train_loss = 0.930\n",
      "2019-02-28T13:06:01.547406: Epoch   2 Batch 1090/3125   train_loss = 1.093\n",
      "2019-02-28T13:06:03.149515: Epoch   2 Batch 1110/3125   train_loss = 1.049\n",
      "2019-02-28T13:06:04.736774: Epoch   2 Batch 1130/3125   train_loss = 0.956\n",
      "2019-02-28T13:06:06.426831: Epoch   2 Batch 1150/3125   train_loss = 0.955\n",
      "2019-02-28T13:06:08.027942: Epoch   2 Batch 1170/3125   train_loss = 0.959\n",
      "2019-02-28T13:06:09.712635: Epoch   2 Batch 1190/3125   train_loss = 1.025\n",
      "2019-02-28T13:06:11.320738: Epoch   2 Batch 1210/3125   train_loss = 0.856\n",
      "2019-02-28T13:06:12.923652: Epoch   2 Batch 1230/3125   train_loss = 0.870\n",
      "2019-02-28T13:06:14.530596: Epoch   2 Batch 1250/3125   train_loss = 0.964\n",
      "2019-02-28T13:06:16.161637: Epoch   2 Batch 1270/3125   train_loss = 1.043\n",
      "2019-02-28T13:06:17.876511: Epoch   2 Batch 1290/3125   train_loss = 0.925\n",
      "2019-02-28T13:06:19.444888: Epoch   2 Batch 1310/3125   train_loss = 0.944\n",
      "2019-02-28T13:06:21.146216: Epoch   2 Batch 1330/3125   train_loss = 1.061\n",
      "2019-02-28T13:06:22.675368: Epoch   2 Batch 1350/3125   train_loss = 0.915\n",
      "2019-02-28T13:06:24.258912: Epoch   2 Batch 1370/3125   train_loss = 0.876\n",
      "2019-02-28T13:06:25.876134: Epoch   2 Batch 1390/3125   train_loss = 1.045\n",
      "2019-02-28T13:06:27.437723: Epoch   2 Batch 1410/3125   train_loss = 0.906\n",
      "2019-02-28T13:06:29.002658: Epoch   2 Batch 1430/3125   train_loss = 0.986\n",
      "2019-02-28T13:06:30.653763: Epoch   2 Batch 1450/3125   train_loss = 0.981\n",
      "2019-02-28T13:06:32.262705: Epoch   2 Batch 1470/3125   train_loss = 1.024\n",
      "2019-02-28T13:06:33.957490: Epoch   2 Batch 1490/3125   train_loss = 0.990\n",
      "2019-02-28T13:06:35.546633: Epoch   2 Batch 1510/3125   train_loss = 1.005\n",
      "2019-02-28T13:06:37.179831: Epoch   2 Batch 1530/3125   train_loss = 1.068\n",
      "2019-02-28T13:06:38.751931: Epoch   2 Batch 1550/3125   train_loss = 0.885\n",
      "2019-02-28T13:06:40.376358: Epoch   2 Batch 1570/3125   train_loss = 0.878\n",
      "2019-02-28T13:06:41.987300: Epoch   2 Batch 1590/3125   train_loss = 0.937\n",
      "2019-02-28T13:06:43.580946: Epoch   2 Batch 1610/3125   train_loss = 1.018\n",
      "2019-02-28T13:06:45.210809: Epoch   2 Batch 1630/3125   train_loss = 1.024\n",
      "2019-02-28T13:06:46.836995: Epoch   2 Batch 1650/3125   train_loss = 0.818\n",
      "2019-02-28T13:06:48.517671: Epoch   2 Batch 1670/3125   train_loss = 0.813\n",
      "2019-02-28T13:06:50.201544: Epoch   2 Batch 1690/3125   train_loss = 0.996\n",
      "2019-02-28T13:06:51.843230: Epoch   2 Batch 1710/3125   train_loss = 0.973\n",
      "2019-02-28T13:06:53.432541: Epoch   2 Batch 1730/3125   train_loss = 1.014\n",
      "2019-02-28T13:06:55.061280: Epoch   2 Batch 1750/3125   train_loss = 0.818\n",
      "2019-02-28T13:06:56.692399: Epoch   2 Batch 1770/3125   train_loss = 1.071\n",
      "2019-02-28T13:06:58.267383: Epoch   2 Batch 1790/3125   train_loss = 1.034\n",
      "2019-02-28T13:07:00.059642: Epoch   2 Batch 1810/3125   train_loss = 0.987\n",
      "2019-02-28T13:07:02.045336: Epoch   2 Batch 1830/3125   train_loss = 1.012\n",
      "2019-02-28T13:07:03.669621: Epoch   2 Batch 1850/3125   train_loss = 0.929\n",
      "2019-02-28T13:07:05.429464: Epoch   2 Batch 1870/3125   train_loss = 1.065\n",
      "2019-02-28T13:07:07.139712: Epoch   2 Batch 1890/3125   train_loss = 0.777\n",
      "2019-02-28T13:07:08.784999: Epoch   2 Batch 1910/3125   train_loss = 0.869\n",
      "2019-02-28T13:07:10.391653: Epoch   2 Batch 1930/3125   train_loss = 0.975\n",
      "2019-02-28T13:07:12.020852: Epoch   2 Batch 1950/3125   train_loss = 0.888\n",
      "2019-02-28T13:07:13.656921: Epoch   2 Batch 1970/3125   train_loss = 0.945\n",
      "2019-02-28T13:07:15.278303: Epoch   2 Batch 1990/3125   train_loss = 0.871\n",
      "2019-02-28T13:07:16.855419: Epoch   2 Batch 2010/3125   train_loss = 0.803\n",
      "2019-02-28T13:07:18.451089: Epoch   2 Batch 2030/3125   train_loss = 0.952\n",
      "2019-02-28T13:07:20.050844: Epoch   2 Batch 2050/3125   train_loss = 0.944\n",
      "2019-02-28T13:07:21.657397: Epoch   2 Batch 2070/3125   train_loss = 0.971\n",
      "2019-02-28T13:07:23.285759: Epoch   2 Batch 2090/3125   train_loss = 0.903\n",
      "2019-02-28T13:07:24.895294: Epoch   2 Batch 2110/3125   train_loss = 1.084\n",
      "2019-02-28T13:07:26.503471: Epoch   2 Batch 2130/3125   train_loss = 1.005\n",
      "2019-02-28T13:07:28.123795: Epoch   2 Batch 2150/3125   train_loss = 0.943\n",
      "2019-02-28T13:07:29.678821: Epoch   2 Batch 2170/3125   train_loss = 0.884\n",
      "2019-02-28T13:07:31.282884: Epoch   2 Batch 2190/3125   train_loss = 0.966\n",
      "2019-02-28T13:07:32.886830: Epoch   2 Batch 2210/3125   train_loss = 0.916\n",
      "2019-02-28T13:07:34.758612: Epoch   2 Batch 2230/3125   train_loss = 0.869\n",
      "2019-02-28T13:07:36.393850: Epoch   2 Batch 2250/3125   train_loss = 1.074\n",
      "2019-02-28T13:07:38.047029: Epoch   2 Batch 2270/3125   train_loss = 0.953\n",
      "2019-02-28T13:07:39.664124: Epoch   2 Batch 2290/3125   train_loss = 0.882\n",
      "2019-02-28T13:07:41.281319: Epoch   2 Batch 2310/3125   train_loss = 0.897\n",
      "2019-02-28T13:07:42.906904: Epoch   2 Batch 2330/3125   train_loss = 1.013\n",
      "2019-02-28T13:07:44.550824: Epoch   2 Batch 2350/3125   train_loss = 1.039\n",
      "2019-02-28T13:07:46.186780: Epoch   2 Batch 2370/3125   train_loss = 0.892\n",
      "2019-02-28T13:07:47.844690: Epoch   2 Batch 2390/3125   train_loss = 1.009\n",
      "2019-02-28T13:07:49.466241: Epoch   2 Batch 2410/3125   train_loss = 1.086\n",
      "2019-02-28T13:07:51.094256: Epoch   2 Batch 2430/3125   train_loss = 0.908\n",
      "2019-02-28T13:07:52.693604: Epoch   2 Batch 2450/3125   train_loss = 0.937\n",
      "2019-02-28T13:07:54.417318: Epoch   2 Batch 2470/3125   train_loss = 1.007\n",
      "2019-02-28T13:07:56.161172: Epoch   2 Batch 2490/3125   train_loss = 1.035\n",
      "2019-02-28T13:07:57.865207: Epoch   2 Batch 2510/3125   train_loss = 1.091\n",
      "2019-02-28T13:07:59.743974: Epoch   2 Batch 2530/3125   train_loss = 0.780\n",
      "2019-02-28T13:08:01.553048: Epoch   2 Batch 2550/3125   train_loss = 1.069\n",
      "2019-02-28T13:08:03.315016: Epoch   2 Batch 2570/3125   train_loss = 0.959\n",
      "2019-02-28T13:08:05.042882: Epoch   2 Batch 2590/3125   train_loss = 0.997\n",
      "2019-02-28T13:08:06.656803: Epoch   2 Batch 2610/3125   train_loss = 1.062\n",
      "2019-02-28T13:08:08.579544: Epoch   2 Batch 2630/3125   train_loss = 0.717\n",
      "2019-02-28T13:08:10.351972: Epoch   2 Batch 2650/3125   train_loss = 0.872\n",
      "2019-02-28T13:08:11.934932: Epoch   2 Batch 2670/3125   train_loss = 1.006\n",
      "2019-02-28T13:08:13.593249: Epoch   2 Batch 2690/3125   train_loss = 0.989\n",
      "2019-02-28T13:08:15.300452: Epoch   2 Batch 2710/3125   train_loss = 0.837\n",
      "2019-02-28T13:08:16.925083: Epoch   2 Batch 2730/3125   train_loss = 1.088\n",
      "2019-02-28T13:08:18.554119: Epoch   2 Batch 2750/3125   train_loss = 0.969\n",
      "2019-02-28T13:08:20.320162: Epoch   2 Batch 2770/3125   train_loss = 0.965\n",
      "2019-02-28T13:08:21.925538: Epoch   2 Batch 2790/3125   train_loss = 0.863\n",
      "2019-02-28T13:08:23.518492: Epoch   2 Batch 2810/3125   train_loss = 0.974\n",
      "2019-02-28T13:08:25.125661: Epoch   2 Batch 2830/3125   train_loss = 0.822\n",
      "2019-02-28T13:08:26.738785: Epoch   2 Batch 2850/3125   train_loss = 0.992\n",
      "2019-02-28T13:08:28.345102: Epoch   2 Batch 2870/3125   train_loss = 0.804\n",
      "2019-02-28T13:08:29.963975: Epoch   2 Batch 2890/3125   train_loss = 0.792\n",
      "2019-02-28T13:08:31.558439: Epoch   2 Batch 2910/3125   train_loss = 1.000\n",
      "2019-02-28T13:08:33.218532: Epoch   2 Batch 2930/3125   train_loss = 0.810\n",
      "2019-02-28T13:08:34.808315: Epoch   2 Batch 2950/3125   train_loss = 1.051\n",
      "2019-02-28T13:08:36.417406: Epoch   2 Batch 2970/3125   train_loss = 0.913\n",
      "2019-02-28T13:08:38.169875: Epoch   2 Batch 2990/3125   train_loss = 0.834\n",
      "2019-02-28T13:08:39.742319: Epoch   2 Batch 3010/3125   train_loss = 0.890\n",
      "2019-02-28T13:08:41.475327: Epoch   2 Batch 3030/3125   train_loss = 0.933\n",
      "2019-02-28T13:08:43.053839: Epoch   2 Batch 3050/3125   train_loss = 0.986\n",
      "2019-02-28T13:08:44.808687: Epoch   2 Batch 3070/3125   train_loss = 0.883\n",
      "2019-02-28T13:08:46.495912: Epoch   2 Batch 3090/3125   train_loss = 0.786\n",
      "2019-02-28T13:08:48.180182: Epoch   2 Batch 3110/3125   train_loss = 0.851\n",
      "2019-02-28T13:08:49.736172: Epoch   2 Batch   18/781   test_loss = 0.761\n",
      "2019-02-28T13:08:50.047178: Epoch   2 Batch   38/781   test_loss = 0.836\n",
      "2019-02-28T13:08:50.311004: Epoch   2 Batch   58/781   test_loss = 0.869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-28T13:08:50.670768: Epoch   2 Batch   78/781   test_loss = 0.911\n",
      "2019-02-28T13:08:50.939592: Epoch   2 Batch   98/781   test_loss = 0.961\n",
      "2019-02-28T13:08:51.204458: Epoch   2 Batch  118/781   test_loss = 0.862\n",
      "2019-02-28T13:08:51.468285: Epoch   2 Batch  138/781   test_loss = 1.008\n",
      "2019-02-28T13:08:51.752567: Epoch   2 Batch  158/781   test_loss = 0.815\n",
      "2019-02-28T13:08:52.014395: Epoch   2 Batch  178/781   test_loss = 0.836\n",
      "2019-02-28T13:08:52.273225: Epoch   2 Batch  198/781   test_loss = 0.938\n",
      "2019-02-28T13:08:52.536052: Epoch   2 Batch  218/781   test_loss = 1.014\n",
      "2019-02-28T13:08:52.801878: Epoch   2 Batch  238/781   test_loss = 0.996\n",
      "2019-02-28T13:08:53.067703: Epoch   2 Batch  258/781   test_loss = 1.013\n",
      "2019-02-28T13:08:53.328531: Epoch   2 Batch  278/781   test_loss = 1.033\n",
      "2019-02-28T13:08:53.594357: Epoch   2 Batch  298/781   test_loss = 0.894\n",
      "2019-02-28T13:08:53.853892: Epoch   2 Batch  318/781   test_loss = 0.883\n",
      "2019-02-28T13:08:54.115376: Epoch   2 Batch  338/781   test_loss = 0.994\n",
      "2019-02-28T13:08:54.379202: Epoch   2 Batch  358/781   test_loss = 0.919\n",
      "2019-02-28T13:08:54.639629: Epoch   2 Batch  378/781   test_loss = 0.856\n",
      "2019-02-28T13:08:54.898636: Epoch   2 Batch  398/781   test_loss = 0.818\n",
      "2019-02-28T13:08:55.160465: Epoch   2 Batch  418/781   test_loss = 1.015\n",
      "2019-02-28T13:08:55.423904: Epoch   2 Batch  438/781   test_loss = 1.040\n",
      "2019-02-28T13:08:55.687731: Epoch   2 Batch  458/781   test_loss = 0.935\n",
      "2019-02-28T13:08:55.954555: Epoch   2 Batch  478/781   test_loss = 0.969\n",
      "2019-02-28T13:08:56.212386: Epoch   2 Batch  498/781   test_loss = 0.810\n",
      "2019-02-28T13:08:56.510191: Epoch   2 Batch  518/781   test_loss = 0.861\n",
      "2019-02-28T13:08:56.775017: Epoch   2 Batch  538/781   test_loss = 0.808\n",
      "2019-02-28T13:08:57.033052: Epoch   2 Batch  558/781   test_loss = 0.807\n",
      "2019-02-28T13:08:57.305873: Epoch   2 Batch  578/781   test_loss = 0.916\n",
      "2019-02-28T13:08:57.568701: Epoch   2 Batch  598/781   test_loss = 1.061\n",
      "2019-02-28T13:08:57.956446: Epoch   2 Batch  618/781   test_loss = 0.855\n",
      "2019-02-28T13:08:58.235263: Epoch   2 Batch  638/781   test_loss = 0.908\n",
      "2019-02-28T13:08:58.498090: Epoch   2 Batch  658/781   test_loss = 1.036\n",
      "2019-02-28T13:08:58.830883: Epoch   2 Batch  678/781   test_loss = 0.957\n",
      "2019-02-28T13:08:59.181641: Epoch   2 Batch  698/781   test_loss = 0.861\n",
      "2019-02-28T13:08:59.456460: Epoch   2 Batch  718/781   test_loss = 1.060\n",
      "2019-02-28T13:08:59.839209: Epoch   2 Batch  738/781   test_loss = 0.830\n",
      "2019-02-28T13:09:00.114800: Epoch   2 Batch  758/781   test_loss = 0.990\n",
      "2019-02-28T13:09:00.380626: Epoch   2 Batch  778/781   test_loss = 0.954\n",
      "2019-02-28T13:09:01.600015: Epoch   3 Batch    5/3125   train_loss = 0.938\n",
      "2019-02-28T13:09:03.635049: Epoch   3 Batch   25/3125   train_loss = 0.983\n",
      "2019-02-28T13:09:05.890555: Epoch   3 Batch   45/3125   train_loss = 0.903\n",
      "2019-02-28T13:09:08.014413: Epoch   3 Batch   65/3125   train_loss = 0.942\n",
      "2019-02-28T13:09:09.990115: Epoch   3 Batch   85/3125   train_loss = 0.813\n",
      "2019-02-28T13:09:11.971812: Epoch   3 Batch  105/3125   train_loss = 0.726\n",
      "2019-02-28T13:09:14.075756: Epoch   3 Batch  125/3125   train_loss = 0.914\n",
      "2019-02-28T13:09:16.175380: Epoch   3 Batch  145/3125   train_loss = 0.922\n",
      "2019-02-28T13:09:17.946210: Epoch   3 Batch  165/3125   train_loss = 0.957\n",
      "2019-02-28T13:09:19.860955: Epoch   3 Batch  185/3125   train_loss = 0.820\n",
      "2019-02-28T13:09:21.580265: Epoch   3 Batch  205/3125   train_loss = 0.811\n",
      "2019-02-28T13:09:23.440235: Epoch   3 Batch  225/3125   train_loss = 0.851\n",
      "2019-02-28T13:09:25.266037: Epoch   3 Batch  245/3125   train_loss = 1.078\n",
      "2019-02-28T13:09:27.144800: Epoch   3 Batch  265/3125   train_loss = 0.912\n",
      "2019-02-28T13:09:29.266438: Epoch   3 Batch  285/3125   train_loss = 0.918\n",
      "2019-02-28T13:09:31.345063: Epoch   3 Batch  305/3125   train_loss = 0.864\n",
      "2019-02-28T13:09:33.270797: Epoch   3 Batch  325/3125   train_loss = 0.931\n",
      "2019-02-28T13:09:35.287199: Epoch   3 Batch  345/3125   train_loss = 0.919\n",
      "2019-02-28T13:09:37.086723: Epoch   3 Batch  365/3125   train_loss = 0.933\n",
      "2019-02-28T13:09:38.958703: Epoch   3 Batch  385/3125   train_loss = 0.859\n",
      "2019-02-28T13:09:40.605783: Epoch   3 Batch  405/3125   train_loss = 0.846\n",
      "2019-02-28T13:09:42.193928: Epoch   3 Batch  425/3125   train_loss = 0.959\n",
      "2019-02-28T13:09:43.783051: Epoch   3 Batch  445/3125   train_loss = 0.921\n",
      "2019-02-28T13:09:45.398582: Epoch   3 Batch  465/3125   train_loss = 0.889\n",
      "2019-02-28T13:09:47.009714: Epoch   3 Batch  485/3125   train_loss = 1.025\n",
      "2019-02-28T13:09:48.611968: Epoch   3 Batch  505/3125   train_loss = 0.869\n",
      "2019-02-28T13:09:50.183688: Epoch   3 Batch  525/3125   train_loss = 1.028\n",
      "2019-02-28T13:09:51.779033: Epoch   3 Batch  545/3125   train_loss = 0.881\n",
      "2019-02-28T13:09:53.427951: Epoch   3 Batch  565/3125   train_loss = 1.131\n",
      "2019-02-28T13:09:55.196251: Epoch   3 Batch  585/3125   train_loss = 0.859\n",
      "2019-02-28T13:09:56.814062: Epoch   3 Batch  605/3125   train_loss = 0.908\n",
      "2019-02-28T13:09:58.606126: Epoch   3 Batch  625/3125   train_loss = 0.913\n",
      "2019-02-28T13:10:00.708741: Epoch   3 Batch  645/3125   train_loss = 0.933\n",
      "2019-02-28T13:10:02.366651: Epoch   3 Batch  665/3125   train_loss = 0.973\n",
      "2019-02-28T13:10:04.090665: Epoch   3 Batch  685/3125   train_loss = 0.928\n",
      "2019-02-28T13:10:05.923460: Epoch   3 Batch  705/3125   train_loss = 1.030\n",
      "2019-02-28T13:10:07.665317: Epoch   3 Batch  725/3125   train_loss = 0.902\n",
      "2019-02-28T13:10:09.400956: Epoch   3 Batch  745/3125   train_loss = 0.890\n",
      "2019-02-28T13:10:11.117151: Epoch   3 Batch  765/3125   train_loss = 0.828\n",
      "2019-02-28T13:10:12.684515: Epoch   3 Batch  785/3125   train_loss = 1.048\n",
      "2019-02-28T13:10:14.294458: Epoch   3 Batch  805/3125   train_loss = 0.815\n",
      "2019-02-28T13:10:16.127055: Epoch   3 Batch  825/3125   train_loss = 0.880\n",
      "2019-02-28T13:10:17.886900: Epoch   3 Batch  845/3125   train_loss = 0.884\n",
      "2019-02-28T13:10:19.730935: Epoch   3 Batch  865/3125   train_loss = 0.984\n",
      "2019-02-28T13:10:21.472863: Epoch   3 Batch  885/3125   train_loss = 0.884\n",
      "2019-02-28T13:10:23.303808: Epoch   3 Batch  905/3125   train_loss = 1.025\n",
      "2019-02-28T13:10:25.153320: Epoch   3 Batch  925/3125   train_loss = 0.881\n",
      "2019-02-28T13:10:26.946894: Epoch   3 Batch  945/3125   train_loss = 0.967\n",
      "2019-02-28T13:10:28.867400: Epoch   3 Batch  965/3125   train_loss = 0.834\n",
      "2019-02-28T13:10:30.481905: Epoch   3 Batch  985/3125   train_loss = 1.003\n",
      "2019-02-28T13:10:32.426814: Epoch   3 Batch 1005/3125   train_loss = 0.817\n",
      "2019-02-28T13:10:34.380529: Epoch   3 Batch 1025/3125   train_loss = 0.932\n",
      "2019-02-28T13:10:36.059611: Epoch   3 Batch 1045/3125   train_loss = 1.099\n",
      "2019-02-28T13:10:37.994337: Epoch   3 Batch 1065/3125   train_loss = 0.900\n",
      "2019-02-28T13:10:39.801686: Epoch   3 Batch 1085/3125   train_loss = 0.811\n",
      "2019-02-28T13:10:41.418746: Epoch   3 Batch 1105/3125   train_loss = 0.838\n",
      "2019-02-28T13:10:43.070114: Epoch   3 Batch 1125/3125   train_loss = 0.904\n",
      "2019-02-28T13:10:44.762002: Epoch   3 Batch 1145/3125   train_loss = 0.899\n",
      "2019-02-28T13:10:46.436276: Epoch   3 Batch 1165/3125   train_loss = 0.949\n",
      "2019-02-28T13:10:48.151294: Epoch   3 Batch 1185/3125   train_loss = 0.823\n",
      "2019-02-28T13:10:49.941470: Epoch   3 Batch 1205/3125   train_loss = 0.871\n",
      "2019-02-28T13:10:51.817838: Epoch   3 Batch 1225/3125   train_loss = 0.997\n",
      "2019-02-28T13:10:53.483744: Epoch   3 Batch 1245/3125   train_loss = 1.097\n",
      "2019-02-28T13:10:55.120075: Epoch   3 Batch 1265/3125   train_loss = 0.919\n",
      "2019-02-28T13:10:57.098830: Epoch   3 Batch 1285/3125   train_loss = 0.996\n",
      "2019-02-28T13:10:58.922706: Epoch   3 Batch 1305/3125   train_loss = 0.819\n",
      "2019-02-28T13:11:00.794878: Epoch   3 Batch 1325/3125   train_loss = 0.837\n",
      "2019-02-28T13:11:02.804560: Epoch   3 Batch 1345/3125   train_loss = 0.934\n",
      "2019-02-28T13:11:04.458625: Epoch   3 Batch 1365/3125   train_loss = 0.803\n",
      "2019-02-28T13:11:06.131681: Epoch   3 Batch 1385/3125   train_loss = 0.808\n",
      "2019-02-28T13:11:07.944488: Epoch   3 Batch 1405/3125   train_loss = 0.881\n",
      "2019-02-28T13:11:09.792297: Epoch   3 Batch 1425/3125   train_loss = 1.041\n",
      "2019-02-28T13:11:11.516271: Epoch   3 Batch 1445/3125   train_loss = 1.044\n",
      "2019-02-28T13:11:13.108015: Epoch   3 Batch 1465/3125   train_loss = 0.905\n",
      "2019-02-28T13:11:14.815088: Epoch   3 Batch 1485/3125   train_loss = 0.954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-28T13:11:16.521134: Epoch   3 Batch 1505/3125   train_loss = 0.769\n",
      "2019-02-28T13:11:18.459860: Epoch   3 Batch 1525/3125   train_loss = 0.792\n",
      "2019-02-28T13:11:20.420572: Epoch   3 Batch 1545/3125   train_loss = 0.917\n",
      "2019-02-28T13:11:22.063782: Epoch   3 Batch 1565/3125   train_loss = 0.854\n",
      "2019-02-28T13:11:23.621759: Epoch   3 Batch 1585/3125   train_loss = 0.871\n",
      "2019-02-28T13:11:25.228192: Epoch   3 Batch 1605/3125   train_loss = 0.999\n",
      "2019-02-28T13:11:26.836896: Epoch   3 Batch 1625/3125   train_loss = 0.972\n",
      "2019-02-28T13:11:28.468220: Epoch   3 Batch 1645/3125   train_loss = 0.971\n",
      "2019-02-28T13:11:30.072209: Epoch   3 Batch 1665/3125   train_loss = 0.940\n",
      "2019-02-28T13:11:31.670800: Epoch   3 Batch 1685/3125   train_loss = 0.987\n",
      "2019-02-28T13:11:33.331877: Epoch   3 Batch 1705/3125   train_loss = 0.955\n",
      "2019-02-28T13:11:34.893012: Epoch   3 Batch 1725/3125   train_loss = 0.829\n",
      "2019-02-28T13:11:36.451156: Epoch   3 Batch 1745/3125   train_loss = 0.783\n",
      "2019-02-28T13:11:38.062756: Epoch   3 Batch 1765/3125   train_loss = 0.845\n",
      "2019-02-28T13:11:39.744258: Epoch   3 Batch 1785/3125   train_loss = 1.030\n",
      "2019-02-28T13:11:41.334441: Epoch   3 Batch 1805/3125   train_loss = 0.943\n",
      "2019-02-28T13:11:43.092439: Epoch   3 Batch 1825/3125   train_loss = 0.978\n",
      "2019-02-28T13:11:44.805313: Epoch   3 Batch 1845/3125   train_loss = 0.969\n",
      "2019-02-28T13:11:46.444127: Epoch   3 Batch 1865/3125   train_loss = 0.784\n",
      "2019-02-28T13:11:48.043239: Epoch   3 Batch 1885/3125   train_loss = 0.970\n",
      "2019-02-28T13:11:49.868339: Epoch   3 Batch 1905/3125   train_loss = 0.843\n",
      "2019-02-28T13:11:51.523783: Epoch   3 Batch 1925/3125   train_loss = 0.808\n",
      "2019-02-28T13:11:53.181106: Epoch   3 Batch 1945/3125   train_loss = 0.921\n",
      "2019-02-28T13:11:54.804418: Epoch   3 Batch 1965/3125   train_loss = 0.830\n",
      "2019-02-28T13:11:56.416059: Epoch   3 Batch 1985/3125   train_loss = 0.838\n",
      "2019-02-28T13:11:57.996635: Epoch   3 Batch 2005/3125   train_loss = 0.895\n",
      "2019-02-28T13:11:59.717323: Epoch   3 Batch 2025/3125   train_loss = 0.949\n",
      "2019-02-28T13:12:01.369653: Epoch   3 Batch 2045/3125   train_loss = 0.802\n",
      "2019-02-28T13:12:03.006052: Epoch   3 Batch 2065/3125   train_loss = 0.725\n",
      "2019-02-28T13:12:04.616854: Epoch   3 Batch 2085/3125   train_loss = 0.984\n",
      "2019-02-28T13:12:06.206999: Epoch   3 Batch 2105/3125   train_loss = 0.920\n",
      "2019-02-28T13:12:07.804705: Epoch   3 Batch 2125/3125   train_loss = 0.963\n",
      "2019-02-28T13:12:09.503906: Epoch   3 Batch 2145/3125   train_loss = 1.024\n",
      "2019-02-28T13:12:11.101136: Epoch   3 Batch 2165/3125   train_loss = 0.893\n",
      "2019-02-28T13:12:12.793577: Epoch   3 Batch 2185/3125   train_loss = 0.929\n",
      "2019-02-28T13:12:14.372540: Epoch   3 Batch 2205/3125   train_loss = 0.968\n",
      "2019-02-28T13:12:15.958570: Epoch   3 Batch 2225/3125   train_loss = 0.866\n",
      "2019-02-28T13:12:17.555522: Epoch   3 Batch 2245/3125   train_loss = 0.787\n",
      "2019-02-28T13:12:19.149211: Epoch   3 Batch 2265/3125   train_loss = 0.922\n",
      "2019-02-28T13:12:20.738383: Epoch   3 Batch 2285/3125   train_loss = 1.086\n",
      "2019-02-28T13:12:22.342781: Epoch   3 Batch 2305/3125   train_loss = 0.800\n",
      "2019-02-28T13:12:23.931738: Epoch   3 Batch 2325/3125   train_loss = 0.825\n",
      "2019-02-28T13:12:25.549353: Epoch   3 Batch 2345/3125   train_loss = 0.839\n",
      "2019-02-28T13:12:27.172126: Epoch   3 Batch 2365/3125   train_loss = 0.714\n",
      "2019-02-28T13:12:28.791196: Epoch   3 Batch 2385/3125   train_loss = 0.957\n",
      "2019-02-28T13:12:30.394400: Epoch   3 Batch 2405/3125   train_loss = 0.892\n",
      "2019-02-28T13:12:32.010488: Epoch   3 Batch 2425/3125   train_loss = 0.869\n",
      "2019-02-28T13:12:33.644698: Epoch   3 Batch 2445/3125   train_loss = 0.927\n",
      "2019-02-28T13:12:35.431201: Epoch   3 Batch 2465/3125   train_loss = 0.791\n",
      "2019-02-28T13:12:37.037454: Epoch   3 Batch 2485/3125   train_loss = 0.838\n",
      "2019-02-28T13:12:38.652625: Epoch   3 Batch 2505/3125   train_loss = 0.823\n",
      "2019-02-28T13:12:40.324327: Epoch   3 Batch 2525/3125   train_loss = 0.866\n",
      "2019-02-28T13:12:41.914282: Epoch   3 Batch 2545/3125   train_loss = 1.023\n",
      "2019-02-28T13:12:43.456261: Epoch   3 Batch 2565/3125   train_loss = 0.840\n",
      "2019-02-28T13:12:45.169136: Epoch   3 Batch 2585/3125   train_loss = 0.837\n",
      "2019-02-28T13:12:46.933250: Epoch   3 Batch 2605/3125   train_loss = 0.842\n",
      "2019-02-28T13:12:48.621616: Epoch   3 Batch 2625/3125   train_loss = 1.056\n",
      "2019-02-28T13:12:50.451486: Epoch   3 Batch 2645/3125   train_loss = 0.865\n",
      "2019-02-28T13:12:52.211133: Epoch   3 Batch 2665/3125   train_loss = 0.940\n",
      "2019-02-28T13:12:54.127930: Epoch   3 Batch 2685/3125   train_loss = 0.928\n",
      "2019-02-28T13:12:56.398431: Epoch   3 Batch 2705/3125   train_loss = 0.813\n",
      "2019-02-28T13:12:58.311526: Epoch   3 Batch 2725/3125   train_loss = 0.969\n",
      "2019-02-28T13:12:59.958718: Epoch   3 Batch 2745/3125   train_loss = 0.864\n",
      "2019-02-28T13:13:01.604819: Epoch   3 Batch 2765/3125   train_loss = 0.813\n",
      "2019-02-28T13:13:03.175780: Epoch   3 Batch 2785/3125   train_loss = 0.980\n",
      "2019-02-28T13:13:04.866802: Epoch   3 Batch 2805/3125   train_loss = 0.841\n",
      "2019-02-28T13:13:06.516876: Epoch   3 Batch 2825/3125   train_loss = 0.860\n",
      "2019-02-28T13:13:08.269308: Epoch   3 Batch 2845/3125   train_loss = 0.826\n",
      "2019-02-28T13:13:09.945569: Epoch   3 Batch 2865/3125   train_loss = 0.833\n",
      "2019-02-28T13:13:11.521781: Epoch   3 Batch 2885/3125   train_loss = 0.927\n",
      "2019-02-28T13:13:13.089383: Epoch   3 Batch 2905/3125   train_loss = 0.931\n",
      "2019-02-28T13:13:14.680338: Epoch   3 Batch 2925/3125   train_loss = 0.866\n",
      "2019-02-28T13:13:16.305596: Epoch   3 Batch 2945/3125   train_loss = 0.940\n",
      "2019-02-28T13:13:17.927531: Epoch   3 Batch 2965/3125   train_loss = 0.981\n",
      "2019-02-28T13:13:19.498155: Epoch   3 Batch 2985/3125   train_loss = 0.816\n",
      "2019-02-28T13:13:21.088976: Epoch   3 Batch 3005/3125   train_loss = 0.819\n",
      "2019-02-28T13:13:22.650045: Epoch   3 Batch 3025/3125   train_loss = 0.926\n",
      "2019-02-28T13:13:24.263285: Epoch   3 Batch 3045/3125   train_loss = 0.963\n",
      "2019-02-28T13:13:25.831909: Epoch   3 Batch 3065/3125   train_loss = 0.831\n",
      "2019-02-28T13:13:27.403373: Epoch   3 Batch 3085/3125   train_loss = 0.876\n",
      "2019-02-28T13:13:28.993529: Epoch   3 Batch 3105/3125   train_loss = 0.869\n",
      "2019-02-28T13:13:30.801696: Epoch   3 Batch   17/781   test_loss = 0.906\n",
      "2019-02-28T13:13:31.075516: Epoch   3 Batch   37/781   test_loss = 0.927\n",
      "2019-02-28T13:13:31.346338: Epoch   3 Batch   57/781   test_loss = 0.949\n",
      "2019-02-28T13:13:31.617160: Epoch   3 Batch   77/781   test_loss = 0.856\n",
      "2019-02-28T13:13:31.881985: Epoch   3 Batch   97/781   test_loss = 0.775\n",
      "2019-02-28T13:13:32.150809: Epoch   3 Batch  117/781   test_loss = 0.993\n",
      "2019-02-28T13:13:32.424631: Epoch   3 Batch  137/781   test_loss = 0.959\n",
      "2019-02-28T13:13:32.702447: Epoch   3 Batch  157/781   test_loss = 0.909\n",
      "2019-02-28T13:13:32.980790: Epoch   3 Batch  177/781   test_loss = 0.892\n",
      "2019-02-28T13:13:33.259606: Epoch   3 Batch  197/781   test_loss = 0.912\n",
      "2019-02-28T13:13:33.537424: Epoch   3 Batch  217/781   test_loss = 0.736\n",
      "2019-02-28T13:13:33.813574: Epoch   3 Batch  237/781   test_loss = 0.755\n",
      "2019-02-28T13:13:34.100385: Epoch   3 Batch  257/781   test_loss = 1.018\n",
      "2019-02-28T13:13:34.386197: Epoch   3 Batch  277/781   test_loss = 0.973\n",
      "2019-02-28T13:13:34.695993: Epoch   3 Batch  297/781   test_loss = 0.998\n",
      "2019-02-28T13:13:34.977971: Epoch   3 Batch  317/781   test_loss = 1.029\n",
      "2019-02-28T13:13:35.270778: Epoch   3 Batch  337/781   test_loss = 0.877\n",
      "2019-02-28T13:13:35.555591: Epoch   3 Batch  357/781   test_loss = 0.898\n",
      "2019-02-28T13:13:35.889372: Epoch   3 Batch  377/781   test_loss = 0.922\n",
      "2019-02-28T13:13:36.238651: Epoch   3 Batch  397/781   test_loss = 0.939\n",
      "2019-02-28T13:13:36.566690: Epoch   3 Batch  417/781   test_loss = 0.820\n",
      "2019-02-28T13:13:36.940445: Epoch   3 Batch  437/781   test_loss = 0.810\n",
      "2019-02-28T13:13:37.251241: Epoch   3 Batch  457/781   test_loss = 0.721\n",
      "2019-02-28T13:13:37.524063: Epoch   3 Batch  477/781   test_loss = 0.908\n",
      "2019-02-28T13:13:37.787888: Epoch   3 Batch  497/781   test_loss = 0.805\n",
      "2019-02-28T13:13:38.052715: Epoch   3 Batch  517/781   test_loss = 0.865\n",
      "2019-02-28T13:13:38.329225: Epoch   3 Batch  537/781   test_loss = 0.857\n",
      "2019-02-28T13:13:38.612039: Epoch   3 Batch  557/781   test_loss = 1.021\n",
      "2019-02-28T13:13:38.907446: Epoch   3 Batch  577/781   test_loss = 0.937\n",
      "2019-02-28T13:13:39.193820: Epoch   3 Batch  597/781   test_loss = 0.868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-28T13:13:39.458646: Epoch   3 Batch  617/781   test_loss = 0.854\n",
      "2019-02-28T13:13:39.718361: Epoch   3 Batch  637/781   test_loss = 0.799\n",
      "2019-02-28T13:13:39.995401: Epoch   3 Batch  657/781   test_loss = 1.054\n",
      "2019-02-28T13:13:40.326183: Epoch   3 Batch  677/781   test_loss = 0.914\n",
      "2019-02-28T13:13:40.586013: Epoch   3 Batch  697/781   test_loss = 0.948\n",
      "2019-02-28T13:13:40.852838: Epoch   3 Batch  717/781   test_loss = 0.875\n",
      "2019-02-28T13:13:41.117664: Epoch   3 Batch  737/781   test_loss = 0.757\n",
      "2019-02-28T13:13:41.389485: Epoch   3 Batch  757/781   test_loss = 1.097\n",
      "2019-02-28T13:13:41.652312: Epoch   3 Batch  777/781   test_loss = 0.906\n",
      "2019-02-28T13:13:42.403096: Epoch   4 Batch    0/3125   train_loss = 0.955\n",
      "2019-02-28T13:13:44.638790: Epoch   4 Batch   20/3125   train_loss = 0.866\n",
      "2019-02-28T13:13:46.294011: Epoch   4 Batch   40/3125   train_loss = 0.910\n",
      "2019-02-28T13:13:47.964048: Epoch   4 Batch   60/3125   train_loss = 0.722\n",
      "2019-02-28T13:13:49.545001: Epoch   4 Batch   80/3125   train_loss = 0.814\n",
      "2019-02-28T13:13:51.126381: Epoch   4 Batch  100/3125   train_loss = 0.926\n",
      "2019-02-28T13:13:52.754548: Epoch   4 Batch  120/3125   train_loss = 0.995\n",
      "2019-02-28T13:13:54.363639: Epoch   4 Batch  140/3125   train_loss = 0.947\n",
      "2019-02-28T13:13:55.982696: Epoch   4 Batch  160/3125   train_loss = 0.747\n",
      "2019-02-28T13:13:57.573610: Epoch   4 Batch  180/3125   train_loss = 0.875\n",
      "2019-02-28T13:13:59.196491: Epoch   4 Batch  200/3125   train_loss = 1.096\n",
      "2019-02-28T13:14:00.811990: Epoch   4 Batch  220/3125   train_loss = 0.886\n",
      "2019-02-28T13:14:02.447914: Epoch   4 Batch  240/3125   train_loss = 0.990\n",
      "2019-02-28T13:14:04.047259: Epoch   4 Batch  260/3125   train_loss = 0.935\n",
      "2019-02-28T13:14:05.787116: Epoch   4 Batch  280/3125   train_loss = 0.991\n",
      "2019-02-28T13:14:07.554906: Epoch   4 Batch  300/3125   train_loss = 1.063\n",
      "2019-02-28T13:14:09.195314: Epoch   4 Batch  320/3125   train_loss = 0.991\n",
      "2019-02-28T13:14:10.927054: Epoch   4 Batch  340/3125   train_loss = 0.725\n",
      "2019-02-28T13:14:12.592797: Epoch   4 Batch  360/3125   train_loss = 0.823\n",
      "2019-02-28T13:14:14.269695: Epoch   4 Batch  380/3125   train_loss = 0.877\n",
      "2019-02-28T13:14:15.878545: Epoch   4 Batch  400/3125   train_loss = 0.849\n",
      "2019-02-28T13:14:17.537454: Epoch   4 Batch  420/3125   train_loss = 0.795\n",
      "2019-02-28T13:14:19.207021: Epoch   4 Batch  440/3125   train_loss = 0.840\n",
      "2019-02-28T13:14:21.084807: Epoch   4 Batch  460/3125   train_loss = 0.861\n",
      "2019-02-28T13:14:22.802909: Epoch   4 Batch  480/3125   train_loss = 1.003\n",
      "2019-02-28T13:14:24.439019: Epoch   4 Batch  500/3125   train_loss = 0.702\n",
      "2019-02-28T13:14:26.087993: Epoch   4 Batch  520/3125   train_loss = 0.912\n",
      "2019-02-28T13:14:27.798571: Epoch   4 Batch  540/3125   train_loss = 0.794\n",
      "2019-02-28T13:14:29.691062: Epoch   4 Batch  560/3125   train_loss = 1.011\n",
      "2019-02-28T13:14:31.415267: Epoch   4 Batch  580/3125   train_loss = 0.928\n",
      "2019-02-28T13:14:33.504138: Epoch   4 Batch  600/3125   train_loss = 0.907\n",
      "2019-02-28T13:14:35.213463: Epoch   4 Batch  620/3125   train_loss = 0.907\n",
      "2019-02-28T13:14:36.863670: Epoch   4 Batch  640/3125   train_loss = 0.863\n",
      "2019-02-28T13:14:38.721678: Epoch   4 Batch  660/3125   train_loss = 0.886\n",
      "2019-02-28T13:14:40.332191: Epoch   4 Batch  680/3125   train_loss = 0.955\n",
      "2019-02-28T13:14:41.999803: Epoch   4 Batch  700/3125   train_loss = 0.927\n",
      "2019-02-28T13:14:43.640846: Epoch   4 Batch  720/3125   train_loss = 0.751\n",
      "2019-02-28T13:14:45.193982: Epoch   4 Batch  740/3125   train_loss = 0.917\n",
      "2019-02-28T13:14:46.816454: Epoch   4 Batch  760/3125   train_loss = 0.764\n",
      "2019-02-28T13:14:48.472538: Epoch   4 Batch  780/3125   train_loss = 0.872\n",
      "2019-02-28T13:14:50.225538: Epoch   4 Batch  800/3125   train_loss = 0.813\n",
      "2019-02-28T13:14:51.975395: Epoch   4 Batch  820/3125   train_loss = 0.894\n",
      "2019-02-28T13:14:53.888138: Epoch   4 Batch  840/3125   train_loss = 0.846\n",
      "2019-02-28T13:14:55.658418: Epoch   4 Batch  860/3125   train_loss = 0.847\n",
      "2019-02-28T13:14:57.221522: Epoch   4 Batch  880/3125   train_loss = 0.791\n",
      "2019-02-28T13:14:58.857482: Epoch   4 Batch  900/3125   train_loss = 0.904\n",
      "2019-02-28T13:15:00.497837: Epoch   4 Batch  920/3125   train_loss = 0.968\n",
      "2019-02-28T13:15:02.086040: Epoch   4 Batch  940/3125   train_loss = 0.899\n",
      "2019-02-28T13:15:03.671987: Epoch   4 Batch  960/3125   train_loss = 0.917\n",
      "2019-02-28T13:15:05.253947: Epoch   4 Batch  980/3125   train_loss = 0.982\n",
      "2019-02-28T13:15:06.821454: Epoch   4 Batch 1000/3125   train_loss = 0.978\n",
      "2019-02-28T13:15:08.453605: Epoch   4 Batch 1020/3125   train_loss = 0.947\n",
      "2019-02-28T13:15:10.049543: Epoch   4 Batch 1040/3125   train_loss = 0.803\n",
      "2019-02-28T13:15:11.706455: Epoch   4 Batch 1060/3125   train_loss = 0.964\n",
      "2019-02-28T13:15:13.340352: Epoch   4 Batch 1080/3125   train_loss = 0.877\n",
      "2019-02-28T13:15:14.973483: Epoch   4 Batch 1100/3125   train_loss = 0.856\n",
      "2019-02-28T13:15:16.564688: Epoch   4 Batch 1120/3125   train_loss = 0.837\n",
      "2019-02-28T13:15:18.203731: Epoch   4 Batch 1140/3125   train_loss = 0.928\n",
      "2019-02-28T13:15:19.845169: Epoch   4 Batch 1160/3125   train_loss = 0.824\n",
      "2019-02-28T13:15:21.433926: Epoch   4 Batch 1180/3125   train_loss = 0.882\n",
      "2019-02-28T13:15:23.043019: Epoch   4 Batch 1200/3125   train_loss = 0.978\n",
      "2019-02-28T13:15:24.844956: Epoch   4 Batch 1220/3125   train_loss = 0.920\n",
      "2019-02-28T13:15:26.374178: Epoch   4 Batch 1240/3125   train_loss = 0.769\n",
      "2019-02-28T13:15:27.992224: Epoch   4 Batch 1260/3125   train_loss = 0.915\n",
      "2019-02-28T13:15:29.568027: Epoch   4 Batch 1280/3125   train_loss = 0.893\n",
      "2019-02-28T13:15:31.112645: Epoch   4 Batch 1300/3125   train_loss = 0.807\n",
      "2019-02-28T13:15:32.720588: Epoch   4 Batch 1320/3125   train_loss = 0.899\n",
      "2019-02-28T13:15:34.359852: Epoch   4 Batch 1340/3125   train_loss = 0.742\n",
      "2019-02-28T13:15:36.015158: Epoch   4 Batch 1360/3125   train_loss = 0.869\n",
      "2019-02-28T13:15:37.606250: Epoch   4 Batch 1380/3125   train_loss = 0.823\n",
      "2019-02-28T13:15:39.253498: Epoch   4 Batch 1400/3125   train_loss = 0.997\n",
      "2019-02-28T13:15:40.859821: Epoch   4 Batch 1420/3125   train_loss = 0.905\n",
      "2019-02-28T13:15:42.737001: Epoch   4 Batch 1440/3125   train_loss = 0.765\n",
      "2019-02-28T13:15:44.358937: Epoch   4 Batch 1460/3125   train_loss = 0.915\n",
      "2019-02-28T13:15:45.988507: Epoch   4 Batch 1480/3125   train_loss = 0.887\n",
      "2019-02-28T13:15:47.588457: Epoch   4 Batch 1500/3125   train_loss = 0.880\n",
      "2019-02-28T13:15:49.182967: Epoch   4 Batch 1520/3125   train_loss = 0.814\n",
      "2019-02-28T13:15:50.821188: Epoch   4 Batch 1540/3125   train_loss = 1.008\n",
      "2019-02-28T13:15:52.460772: Epoch   4 Batch 1560/3125   train_loss = 0.796\n",
      "2019-02-28T13:15:54.075410: Epoch   4 Batch 1580/3125   train_loss = 0.961\n",
      "2019-02-28T13:15:55.928439: Epoch   4 Batch 1600/3125   train_loss = 0.854\n",
      "2019-02-28T13:15:57.528326: Epoch   4 Batch 1620/3125   train_loss = 0.842\n",
      "2019-02-28T13:15:59.129087: Epoch   4 Batch 1640/3125   train_loss = 0.942\n",
      "2019-02-28T13:16:00.745046: Epoch   4 Batch 1660/3125   train_loss = 0.954\n",
      "2019-02-28T13:16:02.405956: Epoch   4 Batch 1680/3125   train_loss = 0.917\n",
      "2019-02-28T13:16:04.008558: Epoch   4 Batch 1700/3125   train_loss = 0.803\n",
      "2019-02-28T13:16:05.602511: Epoch   4 Batch 1720/3125   train_loss = 0.824\n",
      "2019-02-28T13:16:07.237793: Epoch   4 Batch 1740/3125   train_loss = 0.912\n",
      "2019-02-28T13:16:08.813477: Epoch   4 Batch 1760/3125   train_loss = 0.900\n",
      "2019-02-28T13:16:10.421398: Epoch   4 Batch 1780/3125   train_loss = 0.944\n",
      "2019-02-28T13:16:12.064873: Epoch   4 Batch 1800/3125   train_loss = 0.806\n",
      "2019-02-28T13:16:13.643013: Epoch   4 Batch 1820/3125   train_loss = 0.796\n",
      "2019-02-28T13:16:15.316214: Epoch   4 Batch 1840/3125   train_loss = 0.917\n",
      "2019-02-28T13:16:16.886342: Epoch   4 Batch 1860/3125   train_loss = 0.944\n",
      "2019-02-28T13:16:18.519384: Epoch   4 Batch 1880/3125   train_loss = 0.895\n",
      "2019-02-28T13:16:20.142206: Epoch   4 Batch 1900/3125   train_loss = 0.740\n",
      "2019-02-28T13:16:21.714558: Epoch   4 Batch 1920/3125   train_loss = 0.873\n",
      "2019-02-28T13:16:23.423436: Epoch   4 Batch 1940/3125   train_loss = 0.795\n",
      "2019-02-28T13:16:25.327798: Epoch   4 Batch 1960/3125   train_loss = 0.770\n",
      "2019-02-28T13:16:27.051346: Epoch   4 Batch 1980/3125   train_loss = 0.915\n",
      "2019-02-28T13:16:28.625543: Epoch   4 Batch 2000/3125   train_loss = 1.074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-28T13:16:30.217070: Epoch   4 Batch 2020/3125   train_loss = 0.984\n",
      "2019-02-28T13:16:31.807202: Epoch   4 Batch 2040/3125   train_loss = 0.801\n",
      "2019-02-28T13:16:33.405338: Epoch   4 Batch 2060/3125   train_loss = 0.831\n",
      "2019-02-28T13:16:34.988681: Epoch   4 Batch 2080/3125   train_loss = 1.016\n",
      "2019-02-28T13:16:36.590951: Epoch   4 Batch 2100/3125   train_loss = 0.822\n",
      "2019-02-28T13:16:38.198895: Epoch   4 Batch 2120/3125   train_loss = 0.823\n",
      "2019-02-28T13:16:40.015086: Epoch   4 Batch 2140/3125   train_loss = 0.907\n",
      "2019-02-28T13:16:41.700979: Epoch   4 Batch 2160/3125   train_loss = 0.895\n",
      "2019-02-28T13:16:43.338894: Epoch   4 Batch 2180/3125   train_loss = 0.920\n",
      "2019-02-28T13:16:44.937007: Epoch   4 Batch 2200/3125   train_loss = 0.855\n",
      "2019-02-28T13:16:46.561430: Epoch   4 Batch 2220/3125   train_loss = 0.802\n",
      "2019-02-28T13:16:48.222493: Epoch   4 Batch 2240/3125   train_loss = 0.859\n",
      "2019-02-28T13:16:49.975929: Epoch   4 Batch 2260/3125   train_loss = 0.945\n",
      "2019-02-28T13:16:51.644228: Epoch   4 Batch 2280/3125   train_loss = 0.881\n",
      "2019-02-28T13:16:53.272158: Epoch   4 Batch 2300/3125   train_loss = 0.846\n",
      "2019-02-28T13:16:54.938990: Epoch   4 Batch 2320/3125   train_loss = 0.937\n",
      "2019-02-28T13:16:56.619054: Epoch   4 Batch 2340/3125   train_loss = 0.858\n",
      "2019-02-28T13:16:58.412084: Epoch   4 Batch 2360/3125   train_loss = 0.896\n",
      "2019-02-28T13:17:00.589056: Epoch   4 Batch 2380/3125   train_loss = 0.846\n",
      "2019-02-28T13:17:02.695672: Epoch   4 Batch 2400/3125   train_loss = 0.935\n",
      "2019-02-28T13:17:04.564857: Epoch   4 Batch 2420/3125   train_loss = 0.788\n",
      "2019-02-28T13:17:06.208959: Epoch   4 Batch 2440/3125   train_loss = 0.842\n",
      "2019-02-28T13:17:07.766102: Epoch   4 Batch 2460/3125   train_loss = 0.867\n",
      "2019-02-28T13:17:09.374218: Epoch   4 Batch 2480/3125   train_loss = 1.004\n",
      "2019-02-28T13:17:10.978343: Epoch   4 Batch 2500/3125   train_loss = 0.832\n",
      "2019-02-28T13:17:12.593863: Epoch   4 Batch 2520/3125   train_loss = 0.924\n",
      "2019-02-28T13:17:14.361703: Epoch   4 Batch 2540/3125   train_loss = 0.788\n",
      "2019-02-28T13:17:15.946215: Epoch   4 Batch 2560/3125   train_loss = 0.705\n",
      "2019-02-28T13:17:17.489201: Epoch   4 Batch 2580/3125   train_loss = 0.874\n",
      "2019-02-28T13:17:19.098320: Epoch   4 Batch 2600/3125   train_loss = 0.884\n",
      "2019-02-28T13:17:20.706252: Epoch   4 Batch 2620/3125   train_loss = 0.808\n",
      "2019-02-28T13:17:22.317608: Epoch   4 Batch 2640/3125   train_loss = 0.842\n",
      "2019-02-28T13:17:23.929548: Epoch   4 Batch 2660/3125   train_loss = 0.974\n",
      "2019-02-28T13:17:25.564017: Epoch   4 Batch 2680/3125   train_loss = 0.793\n",
      "2019-02-28T13:17:27.175206: Epoch   4 Batch 2700/3125   train_loss = 0.878\n",
      "2019-02-28T13:17:28.731309: Epoch   4 Batch 2720/3125   train_loss = 0.739\n",
      "2019-02-28T13:17:30.305004: Epoch   4 Batch 2740/3125   train_loss = 0.908\n",
      "2019-02-28T13:17:31.873057: Epoch   4 Batch 2760/3125   train_loss = 0.810\n",
      "2019-02-28T13:17:33.473026: Epoch   4 Batch 2780/3125   train_loss = 0.815\n",
      "2019-02-28T13:17:35.103975: Epoch   4 Batch 2800/3125   train_loss = 1.043\n",
      "2019-02-28T13:17:36.681177: Epoch   4 Batch 2820/3125   train_loss = 1.046\n",
      "2019-02-28T13:17:38.225390: Epoch   4 Batch 2840/3125   train_loss = 0.835\n",
      "2019-02-28T13:17:39.857385: Epoch   4 Batch 2860/3125   train_loss = 0.773\n",
      "2019-02-28T13:17:41.478498: Epoch   4 Batch 2880/3125   train_loss = 0.860\n",
      "2019-02-28T13:17:43.541524: Epoch   4 Batch 2900/3125   train_loss = 0.811\n",
      "2019-02-28T13:17:45.281540: Epoch   4 Batch 2920/3125   train_loss = 0.908\n",
      "2019-02-28T13:17:46.931458: Epoch   4 Batch 2940/3125   train_loss = 0.949\n",
      "2019-02-28T13:17:48.523580: Epoch   4 Batch 2960/3125   train_loss = 0.930\n",
      "2019-02-28T13:17:50.129027: Epoch   4 Batch 2980/3125   train_loss = 0.792\n",
      "2019-02-28T13:17:51.727418: Epoch   4 Batch 3000/3125   train_loss = 0.934\n",
      "2019-02-28T13:17:53.334363: Epoch   4 Batch 3020/3125   train_loss = 1.022\n",
      "2019-02-28T13:17:54.861816: Epoch   4 Batch 3040/3125   train_loss = 0.957\n",
      "2019-02-28T13:17:56.451763: Epoch   4 Batch 3060/3125   train_loss = 0.802\n",
      "2019-02-28T13:17:58.234203: Epoch   4 Batch 3080/3125   train_loss = 0.953\n",
      "2019-02-28T13:17:59.872129: Epoch   4 Batch 3100/3125   train_loss = 1.073\n",
      "2019-02-28T13:18:01.537800: Epoch   4 Batch 3120/3125   train_loss = 0.883\n",
      "2019-02-28T13:18:02.124414: Epoch   4 Batch   16/781   test_loss = 0.809\n",
      "2019-02-28T13:18:02.398234: Epoch   4 Batch   36/781   test_loss = 0.924\n",
      "2019-02-28T13:18:02.664060: Epoch   4 Batch   56/781   test_loss = 0.936\n",
      "2019-02-28T13:18:02.933883: Epoch   4 Batch   76/781   test_loss = 1.001\n",
      "2019-02-28T13:18:03.204705: Epoch   4 Batch   96/781   test_loss = 1.005\n",
      "2019-02-28T13:18:03.473528: Epoch   4 Batch  116/781   test_loss = 0.824\n",
      "2019-02-28T13:18:03.737604: Epoch   4 Batch  136/781   test_loss = 0.779\n",
      "2019-02-28T13:18:04.005430: Epoch   4 Batch  156/781   test_loss = 0.911\n",
      "2019-02-28T13:18:04.273253: Epoch   4 Batch  176/781   test_loss = 0.876\n",
      "2019-02-28T13:18:04.539078: Epoch   4 Batch  196/781   test_loss = 0.797\n",
      "2019-02-28T13:18:04.824890: Epoch   4 Batch  216/781   test_loss = 0.957\n",
      "2019-02-28T13:18:05.087717: Epoch   4 Batch  236/781   test_loss = 0.799\n",
      "2019-02-28T13:18:05.360538: Epoch   4 Batch  256/781   test_loss = 0.837\n",
      "2019-02-28T13:18:05.632359: Epoch   4 Batch  276/781   test_loss = 1.061\n",
      "2019-02-28T13:18:05.901183: Epoch   4 Batch  296/781   test_loss = 0.827\n",
      "2019-02-28T13:18:06.168203: Epoch   4 Batch  316/781   test_loss = 0.816\n",
      "2019-02-28T13:18:06.438024: Epoch   4 Batch  336/781   test_loss = 0.738\n",
      "2019-02-28T13:18:06.707044: Epoch   4 Batch  356/781   test_loss = 0.887\n",
      "2019-02-28T13:18:06.967872: Epoch   4 Batch  376/781   test_loss = 0.906\n",
      "2019-02-28T13:18:07.234697: Epoch   4 Batch  396/781   test_loss = 0.876\n",
      "2019-02-28T13:18:07.494527: Epoch   4 Batch  416/781   test_loss = 0.987\n",
      "2019-02-28T13:18:07.765348: Epoch   4 Batch  436/781   test_loss = 0.848\n",
      "2019-02-28T13:18:08.034172: Epoch   4 Batch  456/781   test_loss = 0.735\n",
      "2019-02-28T13:18:08.309229: Epoch   4 Batch  476/781   test_loss = 0.961\n",
      "2019-02-28T13:18:08.580051: Epoch   4 Batch  496/781   test_loss = 0.937\n",
      "2019-02-28T13:18:08.854871: Epoch   4 Batch  516/781   test_loss = 0.759\n",
      "2019-02-28T13:18:09.130152: Epoch   4 Batch  536/781   test_loss = 0.980\n",
      "2019-02-28T13:18:09.398975: Epoch   4 Batch  556/781   test_loss = 0.836\n",
      "2019-02-28T13:18:09.669206: Epoch   4 Batch  576/781   test_loss = 0.978\n",
      "2019-02-28T13:18:09.937031: Epoch   4 Batch  596/781   test_loss = 0.985\n",
      "2019-02-28T13:18:10.202856: Epoch   4 Batch  616/781   test_loss = 0.943\n",
      "2019-02-28T13:18:10.469681: Epoch   4 Batch  636/781   test_loss = 0.846\n",
      "2019-02-28T13:18:10.735506: Epoch   4 Batch  656/781   test_loss = 0.902\n",
      "2019-02-28T13:18:11.004330: Epoch   4 Batch  676/781   test_loss = 1.031\n",
      "2019-02-28T13:18:11.272154: Epoch   4 Batch  696/781   test_loss = 0.829\n",
      "2019-02-28T13:18:11.534983: Epoch   4 Batch  716/781   test_loss = 0.849\n",
      "2019-02-28T13:18:11.803805: Epoch   4 Batch  736/781   test_loss = 1.059\n",
      "2019-02-28T13:18:12.072516: Epoch   4 Batch  756/781   test_loss = 0.855\n",
      "2019-02-28T13:18:12.338342: Epoch   4 Batch  776/781   test_loss = 0.740\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "def get_batches(Xs, ys, batch_size):\n",
    "    for start in range(0, len(Xs), batch_size):\n",
    "        end = min(start + batch_size, len(Xs))\n",
    "        yield Xs[start:end], ys[start:end]\n",
    "        \n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "losses = {'train':[], 'test':[]}\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    \n",
    "    #搜集数据给tensorBoard用\n",
    "    # Keep track of gradient values and sparsity\n",
    "    grad_summaries = []\n",
    "    for g, v in gradients:\n",
    "        if g is not None:\n",
    "            grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name.replace(':', '_')), g)\n",
    "            sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name.replace(':', '_')), tf.nn.zero_fraction(g))\n",
    "            grad_summaries.append(grad_hist_summary)\n",
    "            grad_summaries.append(sparsity_summary)\n",
    "    grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "        \n",
    "    # Output directory for models and summaries\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "     \n",
    "    # Summaries for loss and accuracy\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "    # Inference summaries\n",
    "    inference_summary_op = tf.summary.merge([loss_summary])\n",
    "    inference_summary_dir = os.path.join(out_dir, \"summaries\", \"inference\")\n",
    "    inference_summary_writer = tf.summary.FileWriter(inference_summary_dir, sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    for epoch_i in range(num_epochs):\n",
    "        \n",
    "        #将数据集分成训练集和测试集，随机种子不固定\n",
    "        train_X,test_X, train_y, test_y = train_test_split(features,  \n",
    "                                                           targets_values,  \n",
    "                                                           test_size = 0.2,  \n",
    "                                                           random_state = 0)  \n",
    "        \n",
    "        train_batches = get_batches(train_X, train_y, batch_size)\n",
    "        test_batches = get_batches(test_X, test_y, batch_size)\n",
    "    \n",
    "        #训练的迭代，保存训练损失\n",
    "        for batch_i in range(len(train_X) // batch_size):\n",
    "            x, y = next(train_batches)\n",
    "\n",
    "            categories = np.zeros([batch_size, 18])\n",
    "            for i in range(batch_size):\n",
    "                categories[i] = x.take(6,1)[i]\n",
    "\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5,1)[i]\n",
    "\n",
    "            feed = {\n",
    "                uid: np.reshape(x.take(0,1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2,1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3,1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4,1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1,1), [batch_size, 1]),\n",
    "                movie_categories: categories,  #x.take(6,1)\n",
    "                movie_titles: titles,  #x.take(5,1)\n",
    "                targets: np.reshape(y, [batch_size, 1]),\n",
    "                dropout_keep_prob: dropout_keep, #dropout_keep\n",
    "                lr: learning_rate}\n",
    "\n",
    "            step, train_loss, summaries, _ = sess.run([global_step, loss, train_summary_op, train_op], feed)  #cost\n",
    "            losses['train'].append(train_loss)\n",
    "            train_summary_writer.add_summary(summaries, step)  #\n",
    "            \n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * (len(train_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    time_str,\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    (len(train_X) // batch_size),\n",
    "                    train_loss))\n",
    "                \n",
    "        #使用测试数据的迭代\n",
    "        for batch_i  in range(len(test_X) // batch_size):\n",
    "            x, y = next(test_batches)\n",
    "            \n",
    "            categories = np.zeros([batch_size, 18])\n",
    "            for i in range(batch_size):\n",
    "                categories[i] = x.take(6,1)[i]\n",
    "\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5,1)[i]\n",
    "\n",
    "            feed = {\n",
    "                uid: np.reshape(x.take(0,1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2,1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3,1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4,1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1,1), [batch_size, 1]),\n",
    "                movie_categories: categories,  #x.take(6,1)\n",
    "                movie_titles: titles,  #x.take(5,1)\n",
    "                targets: np.reshape(y, [batch_size, 1]),\n",
    "                dropout_keep_prob: 1,\n",
    "                lr: learning_rate}\n",
    "            \n",
    "            step, test_loss, summaries = sess.run([global_step, loss, inference_summary_op], feed)  #cost\n",
    "\n",
    "            #保存测试损失\n",
    "            losses['test'].append(test_loss)\n",
    "            inference_summary_writer.add_summary(summaries, step)  #\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if (epoch_i * (len(test_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   test_loss = {:.3f}'.format(\n",
    "                    time_str,\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    (len(test_X) // batch_size),\n",
    "                    test_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver.save(sess, save_dir)  #, global_step=epoch_i\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_params((save_dir))\n",
    "\n",
    "load_dir = load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 显示训练loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwQAAAH0CAYAAACD9urSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEBEREREJsf8HIsJEdyJnwTgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2d90d4d50b8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 386
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['train'], label='Training loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 显示测试loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvIAAAH0CAYAAABfKsnMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIfYiBPRERERORDDOSJiIiIiHzo/wN4FKcBose/UQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2d91c4f78d0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 377
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 获取 Tensors\n",
    "使用函数 [`get_tensor_by_name()`](https://www.tensorflow.org/api_docs/python/tf/Graph#get_tensor_by_name)从 `loaded_graph` 中获取tensors，后面的推荐功能要用到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "\n",
    "    uid = loaded_graph.get_tensor_by_name(\"uid:0\")\n",
    "    user_gender = loaded_graph.get_tensor_by_name(\"user_gender:0\")\n",
    "    user_age = loaded_graph.get_tensor_by_name(\"user_age:0\")\n",
    "    user_job = loaded_graph.get_tensor_by_name(\"user_job:0\")\n",
    "    movie_id = loaded_graph.get_tensor_by_name(\"movie_id:0\")\n",
    "    movie_categories = loaded_graph.get_tensor_by_name(\"movie_categories:0\")\n",
    "    movie_titles = loaded_graph.get_tensor_by_name(\"movie_titles:0\")\n",
    "    targets = loaded_graph.get_tensor_by_name(\"targets:0\")\n",
    "    dropout_keep_prob = loaded_graph.get_tensor_by_name(\"dropout_keep_prob:0\")\n",
    "    lr = loaded_graph.get_tensor_by_name(\"LearningRate:0\")\n",
    "    #两种不同计算预测评分的方案使用不同的name获取tensor inference\n",
    "#     inference = loaded_graph.get_tensor_by_name(\"inference/inference/BiasAdd:0\")\n",
    "    inference = loaded_graph.get_tensor_by_name(\"inference/ExpandDims:0\") # 之前是MatMul:0 因为inference代码修改了 这里也要修改 感谢网友 @清歌 指出问题\n",
    "    movie_combine_layer_flat = loaded_graph.get_tensor_by_name(\"movie_fc/Reshape:0\")\n",
    "    user_combine_layer_flat = loaded_graph.get_tensor_by_name(\"user_fc/Reshape:0\")\n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference, movie_combine_layer_flat, user_combine_layer_flat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 指定用户和电影进行评分\n",
    "这部分就是对网络做正向传播，计算得到预测的评分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rating_movie(user_id_val, movie_id_val):\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "    \n",
    "        # Get Tensors from loaded model\n",
    "        uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference,_, __ = get_tensors(loaded_graph)  #loaded_graph\n",
    "    \n",
    "        categories = np.zeros([1, 18])\n",
    "        categories[0] = movies.values[movieid2idx[movie_id_val]][2]\n",
    "    \n",
    "        titles = np.zeros([1, sentences_size])\n",
    "        titles[0] = movies.values[movieid2idx[movie_id_val]][1]\n",
    "    \n",
    "        feed = {\n",
    "              uid: np.reshape(users.values[user_id_val-1][0], [1, 1]),\n",
    "              user_gender: np.reshape(users.values[user_id_val-1][1], [1, 1]),\n",
    "              user_age: np.reshape(users.values[user_id_val-1][2], [1, 1]),\n",
    "              user_job: np.reshape(users.values[user_id_val-1][3], [1, 1]),\n",
    "              movie_id: np.reshape(movies.values[movieid2idx[movie_id_val]][0], [1, 1]),\n",
    "              movie_categories: categories,  #x.take(6,1)\n",
    "              movie_titles: titles,  #x.take(5,1)\n",
    "              dropout_keep_prob: 1}\n",
    "    \n",
    "        # Get Prediction\n",
    "        inference_val = sess.run([inference], feed)  \n",
    "    \n",
    "        return (inference_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[ 4.14291048]], dtype=float32)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_movie(234, 1401)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成Movie特征矩阵\n",
    "将训练好的电影特征组合成电影特征矩阵并保存到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()  #\n",
    "movie_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess:  #\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, movie_combine_layer_flat, __ = get_tensors(loaded_graph)  #loaded_graph\n",
    "\n",
    "    for item in movies.values:\n",
    "        categories = np.zeros([1, 18])\n",
    "        categories[0] = item.take(2)\n",
    "\n",
    "        titles = np.zeros([1, sentences_size])\n",
    "        titles[0] = item.take(1)\n",
    "\n",
    "        feed = {\n",
    "            movie_id: np.reshape(item.take(0), [1, 1]),\n",
    "            movie_categories: categories,  #x.take(6,1)\n",
    "            movie_titles: titles,  #x.take(5,1)\n",
    "            dropout_keep_prob: 1}\n",
    "\n",
    "        movie_combine_layer_flat_val = sess.run([movie_combine_layer_flat], feed)  \n",
    "        movie_matrics.append(movie_combine_layer_flat_val)\n",
    "\n",
    "pickle.dump((np.array(movie_matrics).reshape(-1, 200)), open('movie_matrics.p', 'wb'))\n",
    "movie_matrics = pickle.load(open('movie_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成User特征矩阵\n",
    "将训练好的用户特征组合成用户特征矩阵并保存到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()  #\n",
    "users_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess:  #\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, __,user_combine_layer_flat = get_tensors(loaded_graph)  #loaded_graph\n",
    "\n",
    "    for item in users.values:\n",
    "\n",
    "        feed = {\n",
    "            uid: np.reshape(item.take(0), [1, 1]),\n",
    "            user_gender: np.reshape(item.take(1), [1, 1]),\n",
    "            user_age: np.reshape(item.take(2), [1, 1]),\n",
    "            user_job: np.reshape(item.take(3), [1, 1]),\n",
    "            dropout_keep_prob: 1}\n",
    "\n",
    "        user_combine_layer_flat_val = sess.run([user_combine_layer_flat], feed)  \n",
    "        users_matrics.append(user_combine_layer_flat_val)\n",
    "\n",
    "pickle.dump((np.array(users_matrics).reshape(-1, 200)), open('users_matrics.p', 'wb'))\n",
    "users_matrics = pickle.load(open('users_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_matrics = pickle.load(open('users_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始推荐电影\n",
    "使用生产的用户特征矩阵和电影特征矩阵做电影推荐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 推荐同类型的电影\n",
    "思路是计算当前看的电影特征向量与整个电影特征矩阵的余弦相似度，取相似度最大的top_k个，这里加了些随机选择在里面，保证每次的推荐稍稍有些不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recommend_same_type_movie(movie_id_val, top_k = 20):\n",
    "    \n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "        \n",
    "        norm_movie_matrics = tf.sqrt(tf.reduce_sum(tf.square(movie_matrics), 1, keep_dims=True))\n",
    "        normalized_movie_matrics = movie_matrics / norm_movie_matrics\n",
    "\n",
    "        #推荐同类型的电影\n",
    "        probs_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])\n",
    "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(normalized_movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "        \n",
    "        print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n",
    "        print(\"以下是给您的推荐：\")\n",
    "        p = np.squeeze(sim)\n",
    "        p[np.argsort(p)[:-top_k]] = 0\n",
    "        p = p / np.sum(p)\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = np.random.choice(3883, 1, p=p)[0]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "您看的电影是：[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
      "以下是给您的推荐：\n",
      "3043\n",
      "[3112 \"'Night Mother (1986)\" 'Drama']\n",
      "1040\n",
      "[1054 'Get on the Bus (1996)' 'Drama']\n",
      "624\n",
      "[629 'Rude (1995)' 'Drama']\n",
      "1648\n",
      "[1695 'Artemisia (1997)' 'Drama']\n",
      "3229\n",
      "[3298 'Boiler Room (2000)' 'Drama']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{624, 1040, 1648, 3043, 3229}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_same_type_movie(1401, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 看过这个电影的人还看了（喜欢）哪些电影\n",
    "- 首先选出喜欢某个电影的top_k个人，得到这几个人的用户特征向量。\n",
    "- 然后计算这几个人对所有电影的评分\n",
    "- 选择每个人评分最高的电影作为推荐\n",
    "- 同样加入了随机选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def recommend_other_favorite_movie(movie_id_val, top_k = 20):\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "\n",
    "        probs_movie_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])\n",
    "        probs_user_favorite_similarity = tf.matmul(probs_movie_embeddings, tf.transpose(users_matrics))\n",
    "        favorite_user_id = np.argsort(probs_user_favorite_similarity.eval())[0][-top_k:]\n",
    "    #     print(normalized_users_matrics.eval().shape)\n",
    "    #     print(probs_user_favorite_similarity.eval()[0][favorite_user_id])\n",
    "    #     print(favorite_user_id.shape)\n",
    "    \n",
    "        print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n",
    "        \n",
    "        print(\"喜欢看这个电影的人是：{}\".format(users_orig[favorite_user_id-1]))\n",
    "        probs_users_embeddings = (users_matrics[favorite_user_id-1]).reshape([-1, 200])\n",
    "        probs_similarity = tf.matmul(probs_users_embeddings, tf.transpose(movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "    \n",
    "    #     print(sim.shape)\n",
    "    #     print(np.argmax(sim, 1))\n",
    "        p = np.argmax(sim, 1)\n",
    "        print(\"喜欢看这个电影的人还喜欢看：\")\n",
    "\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = p[random.randrange(top_k)]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "您看的电影是：[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
      "喜欢看这个电影的人是：[[4581 'M' 25 1]\n",
      " [3768 'M' 25 4]\n",
      " [3995 'M' 35 17]\n",
      " [5767 'M' 25 2]\n",
      " [1130 'M' 18 7]\n",
      " [335 'M' 35 18]\n",
      " [5567 'M' 50 3]\n",
      " [2947 'F' 25 0]\n",
      " [1644 'M' 18 12]\n",
      " [52 'M' 18 4]\n",
      " [5254 'M' 18 4]\n",
      " [5728 'F' 35 20]\n",
      " [4870 'F' 35 3]\n",
      " [4667 'M' 25 14]\n",
      " [1745 'M' 45 0]\n",
      " [3839 'M' 50 14]\n",
      " [4085 'F' 25 6]\n",
      " [5669 'M' 56 1]\n",
      " [3031 'M' 18 4]\n",
      " [4800 'M' 18 4]]\n",
      "喜欢看这个电影的人还喜欢看：\n",
      "900\n",
      "[912 'Casablanca (1942)' 'Drama|Romance|War']\n",
      "523\n",
      "[527 \"Schindler's List (1993)\" 'Drama|War']\n",
      "49\n",
      "[50 'Usual Suspects, The (1995)' 'Crime|Thriller']\n",
      "1690\n",
      "[1741 'Midaq Alley (Callej髇 de los milagros, El) (1995)' 'Drama']\n",
      "315\n",
      "[318 'Shawshank Redemption, The (1994)' 'Drama']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{49, 315, 523, 900, 1690}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_other_favorite_movie(1401, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
